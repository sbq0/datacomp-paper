\part*{Appendix}
\tableofcontents

\section{Benchmark rules}
\label{sec:full-competition-rules}

We provide concrete rules below for the two competition tracks that comprise \datanet: filtering and \byod.
Additionally, we provide a checklist, which encourages \users to specify design decisions, hence allowing for more granular comparison between submissions.

\subsection{Filtering track rules}

\begin{itemize}
    \itemsep0em 
    \item Participants can enter submissions for one or many different scales: \texttt{small}, \texttt{medium}, \texttt{large} or \texttt{xlarge}, which represent the raw number of image-text pairs in CommonPool that should be filtered.
    \item After choosing a scale, \users generate a list of uids, where each uid refers to a \pool sample. The list of uids is used to recover image-text pairs from the pool, which is used for downstream CLIP training.
    \item Duplicate uids are allowed.
    \item Participants are \emph{not} allowed to modify the training procedure. Hence, changing hyperparameters, model architecture, optimizer, compute budget, or number of training steps is not allowed. Changing any other training details is also not allowed.
    \item Participants are strongly encouraged to submit and open-source both the list of uids and the code used to generate this list; however, this is not required.
    \item To avoid overfitting, we do not permit running any code or algorithmic dependence on the test images of the evaluation tasks. However, use of other images associated with these tasks (e.g., supervised training sets) is permitted.
    \item Participants can use templates or class labels from the downstream tasks in their filtering algorithms.
\end{itemize}

For clarity, we include some examples of permitted and forbidden uses:
\begin{itemize}
\itemsep0em 
\item[\checkmark] We \textbf{permit} using the ImageNet class label ``triceratops'' in a filtering algorithm.
\item[$\times$] We \textbf{forbid} examining individual or aggregate predictions on the test sets of the evaluation tasks.
\end{itemize}

\subsection{Bring your own data track: amendments}
To facilitate more open-ended exploration, we provide amendments to the Track 1 competition to allow for more diverse submissions in Track 2.
\begin{itemize}
    \itemsep0em 
    \item Participants are allowed to augment \pool data with existing datasets, so long as these data sources do not contain test images from the evaluation tasks. Participants can use data from any \pool; however, they are not required to do so.
    \item Assembling one's own dataset is allowed; however, test images from the evaluation tasks can neither be contained nor otherwise used to construct said dataset. We encourage releasing the image urls or the images themselves in addition to the text for each image. We also encourage rigorous documentation of face-blurring and other data safety checks (see Section~\ref{sec:pool} for more details). We reserve the right to run our own safety code on participant provided data and disqualify entries that do not meet adequate safety standards.
\end{itemize}


\paragraph{Checklist.}
The following checklist provides the basis for more fine-grained comparison between submissions. 
\begin{itemize}
    \item[$\square$] Images from the evaluation tasks are included in my submission. If yes, please specify which datasets.
    \item[$\square$] I used an existing datasets (e.g., YFCC100M~\cite{yfcc100m}) in my submission. If yes, please specify which datasets. (Note: applies to \byod only)
    \item[$\square$] I curated my own data. If yes, please provide (1) image data or urls, (2) text for each image, (3) list of safety steps taken including but not limited to face blurring, explicit content image and text filtering. (Note: applies to \byod only)
\end{itemize}

\section{Contributions}
For this section, contributors are ordered alphabetically.

\subsection{Candidate pool}

\paragraph{Candidate pool lead.} Vaishaal Shankar

\paragraph{Data collection.} Romain Beaumont, Vaishaal Shankar

\paragraph{Pre-processing and metadata.} Giannis Daras, Alex Fang (content filtering lead), Samir Yitzhak Gadre (metadata lead), Ryan Marten (deduplication lead), Vivek Ramanujan, Vaishaal Shankar, George Smyrnis (face blurring lead)

\subsection{Participant tooling}

\paragraph{Participant tooling lead.} Gabriel Ilharco

\paragraph{Resharder.} Romain Beaumont, Yair Carmon, Alex Fang, Jonathan Hayase (lead), Gabriel Ilharco, Vivek Ramanujan, Vaishaal Shankar, Georgios Smyrnis

\paragraph{Training.} Mehdi Cherti, Gabriel Ilharco, Jenia Jitsev, Vivek Ramanujan, Georgios Smyrnis, Mitchell Wortsman (lead)

\paragraph{Evaluation.} Romain Beaumont, Yonatan Bitton, Mehdi Cherti, Dhruba Ghosh (lead), Gabriel Ilharco

\paragraph{Additional infrastructure.} Stephen Mussmann, Sarah Pratt

\subsection{Baselines}

\paragraph{Baselines lead.} Yair Carmon

\paragraph{Filtering track.} Yair Carmon, Rahim Enterazi, Alex Fang, Samir Yitzhak Gadre, Gabriel Ilharco, Kalyani Marathe, Thao Nguyen, Eyal Orgad (co-lead), Georgios Smyrnis, Mitchell Wortsman, Jieyu Zhang (co-lead)

\paragraph{BYOD track.} Gabriel Ilharco, Thao Nguyen

\paragraph{Experiment babysitting.} Alex Fang, Gabriel Ilharco, Samir Yitzhak Gadre

\subsection{Leadership and Advising}

\paragraph{Advising.} Romain Beaumont, Yair Carmon, Alexandros G.\ Dimakis, Ali Farhadi, Hannaneh Hajishirzi, Jenia Jitsev, Pang Wei Koh, Ranjay Krishna, Stephen Mussmann, Sewoong Oh, Alexander Ratner, Olga Saukh, Ludwig Schmidt, Vaishaal Shankar, Shuran Song, Richard Vencu


\paragraph{Leadership.}
Yair Carmon, Alexandros G.\ Dimakis, Jenia Jitsev, Sewoong Oh, Ludwig Schmidt, Vaishaal Shankar

\paragraph{Overall project lead.} Ludwig Schmidt


\section{Additional related work}
\label{sec:more-relatedwork}

Here we expand on Section~\ref{sec:relatedwork}.

Image dataset safety is an active area of research, especially in the context of large-scale dataset construction. In addition to \citet{Birhane2021MultimodalDM}, who study problematic content in LAION-400M, \citet{Yang2019TowardsFD} study the ImageNet dataset and reveal limitations associated with the ImageNet curation strategy---with negative implications for downstream model fairness. \citet{Prabhu2020LargeID} also study the ImageNet dataset and find pornographic content. Both \citet{Birhane2021MultimodalDM} and \citet{Prabhu2020LargeID} survey ethical conundrums and harms that are borne out of improper dataset curation. In an effort to combat dataset toxicity, we conduct NSFW preprocessing (Section~\ref{sec:pool}, Appendix~\ref{app:nsfw}) and blur detected faces (Section~\ref{sec:pool}, Appendix~\ref{app:face}) during pool construction. We also conduct fairness evaluations (Section~\ref{sec:eval-trends}, Appendix~\ref{app:fairness}) for models trained on our data. We hope \pool will be a resource for future work examining dataset safety.

Beyond data selection, \citet{chan2022data} investigate the effects of dataset distribution on emergent properties of transformers, while \citet{fang2022data} look at the relationship between data and model robustness to distribution shifts. We hope our extensive evaluation suite comprised of 38 diverse tasks will facilitate similar studies when training multimodal models at large scale.

Others study how to reduce the burdens of training data annotation in the curation process. Classic approaches include distant supervision~\cite{hoffmann2011knowledge}, crowd-sourced labels~\cite{yuen2011survey}, heuristic rules~\cite{Awasthi2020Learning} and feature annotation~\cite{mann2010generalized}, among others. A recent line of work known as data programming or programmatic weak supervision~\cite{Ratner16,ratner2017snorkel,zhang2021wrench,zhang2022survey} attempts to reduce annotation cost and is found in many industry applications~\cite{bach2019snorkel,Overton}. In data programming, developers write programmatic labeling functions to automatically label a large amount of unlabeled data. The labeling functions could produce noisy and conflicting labels, so researchers have developed methods to aggregate noisy votes to produce the final training labels~\cite{Ratner19,fu2020fast,shin2021universalizing}.

Previous literature also studies methods for training data attribution, which seek to link a model's behavior (e.g., its accuracy on a particular task or subset of data) to particular subsets of its training data. 
Such methods include influence functions, a classic technique from robust statistics \citep{hampel1974influence,cook1977detection} that uses a second-order Taylor expansion to approximate the effect of removing a training point on the learned model parameters
\citep{koh2017understanding,koh2019accuracy,han2020explaining,guo2020fastif}, as well as methods that fit attribution functions directly to the dynamics of repeated training runs \citep{ghorbani2019data,pruthi2020estimating,ilyas2022datamodels,guu2023simfluence}.
Training data attribution methods assume that we have already trained a model, though they can be subsequently used to refine the training data (e.g., by identifying potentially mislabeled training points \citep{koh2017understanding}).
Our focus in this paper is instead on data curation methods---that is, methods for selecting a subset of the training data to train a model in the first place.

In the context of natural language processing, \citet{dataset-cartography} proposes a tool for characterizing samples in a dataset based on training dynamics, labelling instances as ambiguous, easy to learn or hard to learn. Previous literature such as work by \citet{le2020adversarial,li2019repair,gururangan-etal-2018-annotation} advocate for removing easy instances from the training data. 
\citet{ethayarajh2022understanding} propose a measure of how difficult a dataset is to learn, $\mathcal{V}$-usable information. Such techniques could be promising directions of further exploration in the context of our benchmark.

Finally, another related line of work is studying scaling trends. In addition to \citet{sorscher2022beyond}, researchers have investigated how model performance changes as a function of compute budget, model size, and number of training samples \cite{kaplan2020scaling,chinchilla,caballero2022broken,cherti2022reproducible}.
However, this line of work does not consider how dataset design may affects scaling trends. 
Beyond dataset size, we measure the effects of different dataset sources and filtering strategies.
While scaling trends are central to our investigations, the purpose of our benchmark is to search for the next generation of large multimodal datasets to facilitate more accurate and reliable models.

\section{Parsing Common Crawl}
\label{app:parse-cc}
Common Crawl releases metadata files for the websites that they index (i.e., WAT files). They release these files approximately once a month. We consider all files available from 2014 through November of 2022. We first parse these files, utilizing Apache Spark~\cite{zaharia2016apache} to extract image urls and corresponding alt-text. We map each url, text pair to a uid hash and remove duplicates. This results in 88 billion url, text pairs, which are randomized via a distributed shuffle. Note, we do not consider image content when running uid deduplication at this step. Hence, two identical images with different urls and the same caption would both be retained.

\section{Not safe for work (NSFW) filtering}
\label{app:nsfw}
Our data is sourced from Common Crawl, which contains snapshots of the web. Therefore, we apply multiple layers of NSFW content filtering to remove problematic images and captions from \pool.

First, we filter our captions with Detoxify \citep{Detoxify}, a language model for toxic comment classification. Specifically, we use the multilingual XLM-RoBERTa~\cite{Conneau2019UnsupervisedCR} variant. The model outputs scores between zero and one for the following categories: toxicity, severe toxicity, obscene, identity attack, insult, threat, and sexually explicit. As we had no ground truth for our data, we manually inspected a 1 million random subset of \pool at varying thresholds. We found that a threshold of 0.1 provided good coverage of filtering out NSFW text.
If any of the detoxify category scores exceeds the threshold, the sample is discarded.
Qualitatively, we found that the model struggled with multilingual content, acronyms, and innuendo.
Even at 0.1, we noticed there are some captions that are NSFW.
However, lowering the threshold further heavily affected false positives. We therefore use a 0.1 threshold for all NSFW categories, which on a random subset of one million captions achieves positive rates shown in Table~\ref{tab:detoxify}. 

\begin{table*}
\renewcommand{\arraystretch}{1.1}
\rowcolors{2}{white}{light-light-gray}
    \caption{Detoxify positive rates by threshold on 1 million caption subset of Common Crawl.}
    {
    \centering
    \resizebox{\textwidth}{!}{

    \begin{tabular}{cccccccc}
    \toprule
    Threshold & Toxicity & Severe Toxicity & Obscene & Identity Attack & Insult & Threat & Sexual Explicit   \\ \midrule
    0.01       & 9.5\%   & 1.0\%     & 33.4\% & 1.8\% & 35.0\% & 1.3\% & 2.0\% \\
    % \midrule
    0.1       & 3.6\%   & 0.1\%     & 0.8\% & 0.3\% & 1.4\% & 0.1\% & 1.0\% \\ \bottomrule
    \end{tabular}}\par
    }
    \label{tab:detoxify}
\end{table*}

Second, on the vision side, we use a modified version of LAION-5B's \citep{laion5b} CLIP-based binary classification NSFW model, which takes CLIP ViT-L/14 visual embeddings as input. We remove the initial multi-category encoder from the model, and retrain on the same data with an initial normalization layer followed by a 4-layer multilayer perceptron. Our retrained model matches the performance of the original model on their manually annotated testset. Specifically, we achieve 97.4\% classification accuracy on a held out test set compared to  96.1\% for the original LAION NSFW image filtering model. Additional details about the training data can be found in Appendix C.5 of the LAION-5B paper. In brief, the training data contains 682K images that is roughly balanced with images from safe for work and NSFW categories.

\begin{table*}
\renewcommand{\arraystretch}{1.1}
\rowcolors{3}{light-light-gray}{white}
    \caption{Comparing LAION-2B CLIP based NSFW filtering model to Google Vision API Safe Search adult category on a 40,000 random subset of Common Crawl.}
    {
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
     & False Positive Rate & True Positives &  &      \\ 
   \multirow{-2}{*}{Threshold} &  (Relative to Google) & (Manual Review) & \multirow{-2}{*}{Model Positive Rate} & \multirow{-2}{*}{Google API Positive Rate} \\\midrule
    
    0.1       & 3.6\%  &      2       & 14.4\%                & 3.5\% \\
    0.2       & 0.6\%  &      2       & 9.1\%                 & 3.5\% \\
    0.3       & 0.3\%  &      3       & 7.2\%                 & 3.5\% \\ \bottomrule
    \end{tabular}}\par
    }
    \label{tab:nsfw}
\end{table*}

To evaluate our model and determine a threshold, we used Google Vision API's SafeSearch explicit content detector to generate labels for an 40,000 random subset of our candidate pool. Specifically, an image is NSFW if SafeSearch classifies it as likely or very likely adult (i.e., sexually explicit). As shown in Table~\ref{tab:nsfw}, we found that by thresholding at 0.1 we achieve high recall relative to SafeSearch and very few true positives after manual review. We also manually reviewed images classified by SafeSearch as likely or very likely racy and found that the images were either benign, subjectively suggestive but not explicit, or already found in the set of images labeled as adult.

\section{Deduplication against evaluation sets}
\label{app:dedup}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/dedups.pdf}
    \caption{ Candidate images (top) that are detected as duplicates against images in the evaluation sets (bottom) are removed from the pool. In addition to exact duplicate images, near-duplicates with variable aspect ratios, JPEG compression, overlays, color adjustment, and artistic rendering are also detected. }
    \label{fig:dedups}
\end{figure*}

To prevent data leakage, we filter \pool by removing duplicate and near-duplicate matches of evaluation set images. See Figure \ref{fig:dedups} for example query images from Common Crawl and corresponding near-duplicates in our evaluations sets. We consider images as duplicates when the cosine similarity between a query (Common Crawl image) feature and a reference (evaluation image) feature is higher than a fixed threshold. We employ the deduplication model proposed by \citet{Yokoo2021Dedup}, which earned 1st place in the Facebook AI Image Similarity Challenge (ISC) \cite{douze2021isc}. We choose a cosine similarity threshold of 0.604169 to maximize the true duplicates detected, without removing too many false duplicates from the pool. We compare against OpenAI's CLIP ViT-B/32 as a baseline on ISC. We find that for our threshold, the ISC model achieves precision 0.9 and recall 0.8. At a threshold of 0.96, CLIP achieves the same precision 0.9, but a significantly worse recall of 0.02. Approximately 2.8\% of downloaded samples are flagged as evaluation set near-duplicates.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\linewidth]{figures/pr_curve_all_.pdf}
    \includegraphics[width=0.48\linewidth]{figures/pr_curve_all_clip.pdf}
    \caption{Analysis of different de-duplication strategies across a variety of image transformations. We see that the model introduced by \citet{Yokoo2021Dedup} is better in almost every transformation, with the exception of very aggressive aspect ratio modification.}
    \label{fig:dedup-analysis}
\end{figure}

To verify the performance of our de-duplication models with greater granularity, we modify the evaluation procedure in \citet{douze2021isc} to include transformations which are representative of naturally-occurring duplications on the Internet. Specifically, we study: 1) jpeg compression (encoding), 2) image flips, 3) image rotations, 4) aspect ratio modifications, and 5) grayscaling. To do this, we sample 20\% of the images from each of our evaluation datasets uniformly at random to serve as a reference set of about 140,000 images. Next we sample 560,000 images uniformly at random from LAION-2B to serve as distractors, for a 4-to-1 distractor to reference ratio. Finally, we apply each of the augmentations above and use threshold filtering to determine duplicates. Figure~\ref{fig:dedup-analysis} shows the results from the deduplication model \cite{Yokoo2021Dedup} compared with OpenAI's CLIP ViT-L/14. At high recall values, we see that CLIP filtering results in removing over 2$\times$ the data as that of the deduplication model from \citet{Yokoo2021Dedup}.

\section{Face blurring}
\label{app:face}

As an extra step to safeguard against issues of privacy that may arise from the use of data scraped from the web, we include face blurring as part of our pool creation. To create face metadata, we use the SCRFD face detector \cite{guo2021sample} to extract bounding boxes for the faces in our images. These bounding boxes are included as part of the image metadata in our pool. We make use of the pretrained SCRFD-10G model. We use the same preprocessing as the one described in the official repository of the paper, with the exception of providing $224 \times 224$ input images (by padding each image to square and then resizing) to limit computation costs. Invoking this model provides us with bounding boxes along with an associated score, which we then compare against a threshold of $0.3$ to keep or discard this bounding box. This threshold is the default one used in the repository of SCRFD for the visualization of bounding boxes, and we found it to perform well on our data as discussed next.

In Table \ref{tab:face_detection} we can see the result of face detection on a set of 3293 images from \pool. We evaluate the detection on whether the image has visible faces or not (where images such as cartoon drawings of non-real human faces are not considered as positives), and whether the detector has detected these visible faces. We considered an image as a true positive if all the clearly visible faces in the image were detected, based on the above thresholding process.
We did not do extensive box labeling. True positives are instead determined by human inspection.
We compare the quality of these detections with the Amazon Rekognition system, which is the one upon which the face detections on ImageNet were based \cite{yang2022study}. Note that in this scenario, the recall of the detectors is more important than precision (as detecting a few more bounding boxes across our pool does not affect privacy).

\begin{table*}
\renewcommand{\arraystretch}{1.1}
\rowcolors{2}{white}{light-light-gray}
    \centering
    \caption{Face detection performance on a set of 3293 random images from \pool.}
    \label{tab:face_detection}
    \begin{tabular}{lrr}
    \toprule
    ~ & SCRFD-10G & Amazon Rekognition \\ \midrule
    Accuracy & 93.87 & 96.57 \\
    Precision & 75.87 & 86.09 \\
    Recall & 90.53 & 93.75 \\ \bottomrule
    \end{tabular}
\end{table*}

To utilize these bounding boxes on our data, we apply a standard blurring pipeline, as proposed by \citet{yang2022study}. The result of this process is an image where the faces is blurred and there is a smooth transition from blurred to clean parts of the image.
In Figure \ref{fig:faces} we see the distribution of faces for the {\small \texttt{small}} \pool. Note that the majority of images do not contain faces.

\begin{figure*}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/appx-faces.pdf}
    \caption{Frequency of predicted number of faces in the {\small \texttt{small}} \pool.}
    \label{fig:faces}
    % \vspace{-15mm}
\end{figure*}

As part of our competition pipeline, images are by default blurred during the download process. In Table \ref{tab:face_blur_test} we can see the results of training on 100M images with and without the application of face blurring, as provided by our detector. We can see that the difference in performance is small, which suggests that the application of face blurring does not significantly affect the performance on our downstream tasks.

Finally, we evaluated the detector we used for potential biases. More specifically, we used the detector on the validation set of the FairFace dataset \cite{karkkainen2021fairface}. We found that the central face of the image was detected in all the images of the validation set, regardless of subgroup.

\begin{table*}
    \centering
    \renewcommand{\arraystretch}{1.1}
    \rowcolors{2}{white}{light-light-gray}
    \caption{Effect of face blurring on zero-shot performance. Face blurring improves the privacy preservation of our dataset, while affecting model performance negligibly. Results shown for the {\small \texttt{medium}} scale.}
    \label{tab:face_blur_test}
    \resizebox{\textwidth}{!}{

    \begin{tabular}{lccc}
    \toprule
    Filtering & Face blurring & ImageNet acc. & Avg. performance \\ \midrule
    \cellcolor{white} & $\times$ & 0.209 & 0.246 \\
    \cellcolor{white}\multirow{-2}{*}{CLIP score (B/32, thresh. 0.3) + English filtering} & \checkmark & 0.196 & 0.243 \\ \midrule
   \cellcolor{white} & $\times$ & 0.287 & 0.301 \\
   \cellcolor{white} \multirow{-2}{*}{CLIP score (B/32, 30\%)}  & \checkmark & 0.282 & 0.298 \\ \bottomrule
    \end{tabular}}
\end{table*}

\section{\datanet \pool creation pipeline}
\label{app:metadata}


\begin{table*}[t]
\small
    \centering
    \renewcommand{\arraystretch}{1.1}
        \rowcolors{2}{white}{light-light-gray}
    \caption{Provided metadata for \pool.}
    {
    \centering
    \begin{tabular}{llr}
    \toprule
    Generation Time & Label & Additional notes  \\ \midrule
     \cellcolor{white}& uid & ~ \\
    \cellcolor{white} & url & Link to the image. \\
    \cellcolor{white} & text & Image caption. \\
    \cellcolor{white}& original\_width & ~ \\
    \cellcolor{white} & original\_height & ~ \\
    \multirow{-6}{*}{Step 2} & sha256 & Safeguard for data poisoning. \\ \midrule
    \cellcolor{white} & clip\_b32\_similarity\_score & ~ \\
    \cellcolor{white} & clip\_b32\_image\_features & In separate file.\\
    \cellcolor{white} & clip\_b32\_text\_features & In separate file. \\~ & clip\_l14\_similarity\_score & ~ \\
    \cellcolor{white} & clip\_l14\_image\_features & In separate file. \\~ & clip\_l14\_text\_features & In separate file.\\
    \cellcolor{white}\multirow{-7}{*}{Step 1} & face\_bboxes & ~ \\ \midrule
    \cellcolor{white} & nsfw\_image\_score & ~ \\
    \cellcolor{white} & nsfw\_text\_score & ~ \\
    \cellcolor{white}\multirow{-3}{*}{Step 2, dropped during Step 3} & dedup\_score & ~ \\ \bottomrule
    \end{tabular}
    }
    \label{tab:app_metadata}
\end{table*}
\FloatBarrier

Creating \pool was a multistep process, which involved (1) parsing image urls and alt-text from Common Crawl dumps and downloading these images, (2) tagging images with metadata and (3) conducting safety content filtering and evaluation set duplication. In this section we provide an overview of the data pipeline used to create \pool. For an overview of our ``data funnel'' see Figure \ref{fig:data_pipeline}.

\begin{enumerate}
    \item For the first step, we use parse Common Crawl metadata files to harvest image-text pairs (Section \ref{app:parse-cc}). We use \texttt{img2dataset}\footnote{\url{https://github.com/rom1504/img2dataset}} to obtain $\sim$16.8B downloaded samples. This is the first, unfiltered version of \pool, and contains only basic information for our images (i.e., the original image height, width, and alt-text caption). During this step we also resize images such that their largest dimension does not exceed 512px.
    This eases storage requirements for large images, but is still larger than the 224px resolution used for later training stages.
    
    \item For the second step, we process our unfiltered pool and create richer metadata for each image-text pair. We generate the following for each sample:
    \begin{itemize}
        \item CLIP ViT-B/32 and CLIP ViT-L/14 image and text features, with their associated similarities.
        \item NSFW scores for the image and the text, using the analysis described in Appendix \ref{app:nsfw}.
        \item Deduplication score for the image, as described in Appendix \ref{app:dedup}.
        \item Bounding boxes for faces detected in the image, using the method described in Appendix \ref{app:face}.
    \end{itemize}

    \item For the third and final step, we filter our image-text pairs based on the metadata generated during the second stage. We filter out image-text pairs where the NSFW and deduplication scores exceed the respective thresholds (Section \ref{app:nsfw}). From the images that pass through this filtering, we keep only the desired amount (e.g., 12.8B images from the {\small \texttt{xlarge}} \pool).
    Smaller pools are telescoping subsets of larger pools.
    We package the metadata and image urls, which is made publicly available to the \users.
    Note, we do not release raw image data but rather image urls pointing to images.
\end{enumerate}

A summary of the metadata for each sample is found in Table \ref{tab:app_metadata}. 
To validate our pipeline for duplication and CLIP feature correctness, we also take ImageNet train though metadata generation as a unit test. Using the deduplication features, we detect that 100\% of the images are in fact duplicates. Additionally using the CLIP ViT-B/32 and CLIP ViT-L/14 image features and corresponding text features from OpenAI's 80-prompt ensemble, we achieve 63.36\% and 75.54\% top-1 accuracies, which match the performance reported in the CLIP paper~\cite{radford2021learning}.

When creating pools of different scale (i.e., number of samples), we ensure that smaller pools are subsets of larger pools. For instance, the {\small \texttt{small}} \pool is a subset of the {\small \texttt{xlarge}} \pool.

After \pool is created, the \users can then download the final image-text pairs using the provided files via \texttt{img2dataset}. To further ease the computational burden on \users, we additionally provide metadata for each sample in \pool. Note that when downloading, our \texttt{img2dataset} configuration automatically blurs faces. Hence this is an automatic step on not something \users must do ad hoc.

\section{\pool statistics}
\label{app:pool-stats} 

To provide more information about the kinds of samples in our \pool, we conduct additional analysis on the {\small \texttt{small}} pool, which is an i.i.d. sample of downloaded data and a subset of the larger pools.

In Figure \ref{fig:clip_sim} we show CLIP similarity similarity scores between images and their corresponding text. We notice a flatter distribution of CLIP ViT-L/14 scores than corresponding B/32 scores.

Turning our attention to images in \pool, in Figure \ref{fig:image_stats}, we visualize the aspect ratios and sizes of original images (i.e., before they are downloaded and resized).
In Figure \ref{fig:resize_distrib}, we display a distribution of image height and width after \emph{download} resizing. Notice that the majority of images are around $224 \times 224$px, which is the final resized resolution used for training.

Analysing the textual component of each sample, we visualize frequency of the number of CLIP BPE tokens in the captions (Figure \ref{fig:tokens}) and most common languages (Figure \ref{fig:langdet_language}).
Token counts follow a long-tailed distribution with much more mass in the short sequence range, while English is the predominant language in \pool according to fasttext and cld3.

We also look at url statistics. In Figure \ref{fig:domain} we see common domain names in \pool (e.g., wordpress domains) and common suffixes (e.g., .com or .net).

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/appx-clip_sim.pdf}
    \caption{Image-text similarity score distributions using CLIP ViT-B/32 \emph{(left)} and ViT-L/14 \emph{(right)} models. We plot samples from the \texttt{small} \pool, which are an i.i.d. sample of the \texttt{xlarge} \pool.}
    \label{fig:clip_sim}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/appx-image_statistics.pdf}
    \caption{Statistics for images in the \texttt{small} \pool, before applying resizing.}
    \label{fig:image_stats}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.4\linewidth]{figures/appx-img_distrib.pdf}
    \caption{\textbf{Image pixel heatmap.} Each entry in the above heatmap represents the estimated probability that a pixel is occupied. The center entry has a value of 1.0 as every image has a center pixel. We compute the heatmap over the {\small \texttt{small}} \pool. Note that image sizes are bounded as we resize all images such that their max dimension does not exceed 512px during dataset download.}
    \label{fig:resize_distrib}
    % \vspace{-15mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/appx-tokens.pdf}
    \caption{Distribution of token length for alt-text in the {\small \texttt{small}} \pool. The CLIP BPE tokenizer is used for tokenization.}
    \label{fig:tokens}
    % \vspace{-15mm}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/appx-langdet.pdf}
    \caption{Counts for the top 25 most frequent languages in the {\small \texttt{small}} \pool, as predicted by fasttext \emph{(left)} and cld3 (\emph{right}).}
    \label{fig:langdet_language}
    % \vspace{-15mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/appx-domain.pdf}
    \caption{Counts for the top 25 most frequent domains \emph{(left)} and suffixes (\emph{right}) in the {\small \texttt{small}} \pool.}
    \label{fig:domain}
    % \vspace{-15mm}
\end{figure*}

\FloatBarrier



\section{Efficient training on data subsets}
\label{app:resharder}

When training at large scale, it is important to use efficient access patterns to load training data.
This typically means that data must be loaded using large sequential reads instead of random reads in order to maximize throughput.
In \datanet{}, this is facilitated by the WebDataset\footnote{\url{https://github.com/webdataset/webdataset}} format which stores the training examples in tar files (called ``shards'') and WebDataLoader which makes it easy to load data stored in this format.

Given an arbitrary subset of a pool, we would like to efficiently train on that subset.
Because WebDataset format does not permit efficient random access (a feature inherited from tar), we must read through the entire pool to select the required images.
There are two ways to implement this filtering:
\begin{enumerate}
    \item \textbf{Filter during training:} we apply a predicate during training data loading that discards data not present in the subset.\label{app:resharder:enum-train}
    \item  \textbf{Filter before training:} we iterate over the pool, selecting the images in the subset, and write them to a new WebDataset.\label{app:resharder:enum-reshard}
\end{enumerate}
After some profiling, we concluded that option~\ref{app:resharder:enum-train} had too much overhead in the case where the subset is much smaller than the pool. To see why, note that if the subset is an \(p\)-fraction of the pool size, then we would end up reading a \(1/p\) factor more data than needed for training.
Instead, we give an implementation of option~\ref{app:resharder:enum-reshard}, which performs at most twice as many reads as needed for training.\footnote{Since in \datanet{}, the number of examples seen is equal to the pool size.}

Our tool, called the \textit{resharder}, reads a set of uids in NumPy array format, scans through the pool, selecting those examples, and writes them to a new WebDataset.
The resharder uses multiprocessing to make good use of hardware and can be distributed over many computers to further increase throughput.
The resharder also supports streaming data to and from cloud storage such as Amazon S3.
The resharder is provided to participants as part of the competition tooling.

\section{Effect of duplicates in the training data}
\label{app:dedup_training}

Given that \pool was constructed by scraping the web for image and text pairs, there is a likelihood that some of our images are duplicates of each other, even if they originated from different web sources and have different captions. Here we examine the effect of removing such duplicates. We used the technique proposed by \citet{webster2023deduplication}, where CLIP image features are first compressed and then used to do an approximate nearest neighbor search. After this process, two images $x$ and $y$ are considered duplicates if $\frac{|d_{ADC}(x,x) - d_{ADC}(x,y)|}{d_{ADC}(x,x)} < T_{ADC}$, where $T_{ADC}$ is some threshold and $d_{ADC}(x,x)$ is the distance of a vector with its quantized version used for approximate nearest neighbor search. For each image, we search duplicates across its $1000$ nearest neighbors, and keep it if it's the one with the highest CLIP ViT-L/14 similarity score across its duplicates. Results can be seen in Table \ref{tab:app_dedup_training}, both when this technique is used by itself and in conjunction with ViT-B/32 filtering. We can see that the there are small improvements over using CLIP filtering by itself with respect to the average performance across evaluation datasets.

\begin{table*}
\caption{Effect of deduplication of training set for the medium size \pool. The filtering performed here is CLIP B32 score top 30\% (see Table \ref{tab:full-medium}). Higher threshold values lead to more samples being labeled as duplicates.}
\setlength\tabcolsep{4pt}
    \renewcommand{\arraystretch}{1.1}
        \rowcolors{2}{white}{light-light-gray}
\small
\centering
\begin{tabular}{lccc}
\toprule
Subset & Training dataset size & ImageNet accuracy & Average performance \\
\midrule
$T_{ADC} = 0.1$, without filtering & 99.8M & 0.195 & 0.272 \\
$T_{ADC} = 0.2$, without filtering & 85.9M & 0.200 & 0.274\\
$T_{ADC} = 0.5$, without filtering & 29.6M & 0.227 & 0.292\\
$T_{ADC} = 0.1$, with filtering & 33.5M & 0.288 & 0.333 \\
$T_{ADC} = 0.2$, with filtering & 30.6M & 0.289 & 0.333 \\
$T_{ADC} = 0.5$, with filtering & 15.5M & 0.252 & 0.307 \\
\bottomrule
\end{tabular}
\label{tab:app_dedup_training}
\end{table*}

\section{Training with additional steps}
\label{app:steps}
Recall that one of our major design decisions for \datanet is to fix the hyperparameters associated with model training, following closely hyperparameters from prior work \cite{radford2021learning}. We choose to fix hyperparameters to place emphasis on data curation and remove confounders arising from hyperparameter differences between \users. Here we ablate our hyperparameter configuration by training {\small \texttt{small}} baselines for 10$\times$ more steps. In Figure \ref{fig:steps} we see positive correlation for ImageNet accuracy for the ablated and original hyperparameter configurations. We see similar correlation for average performance. See Table \ref{tab:app_steps} for specific values.

\begin{figure*}
    \centering
    \includegraphics[width=.9\textwidth]{figures/appx-steps.pdf}
    \caption{\emph{(left)} The effect of training for 10$\times$ steps for for {\small \texttt{small}} filtering track baselines on ImageNet. \emph{(right)} Similar plot but for Avg. performance. While the ordering of some methods changes quite drastically, we, in general, see a positive correlation.}
    \label{fig:steps}
    % \vspace{-15mm}
\end{figure*}

\begin{table*}

\setlength\tabcolsep{4pt}
\renewcommand{\arraystretch}{1.1}
        \rowcolors{3}{light-light-gray}{white}
\small
\centering
\caption{Experiment details when extending the number of steps by 10 times the standard amount for that scale.}
\resizebox{\textwidth}{!}{
\begin{tabular}{llccccc}
\toprule

\multirow{2}{*}{Scale} & \multirow{2}{*}{Filtering} & \multirow{2}{*}{ImageNet} & ImageNet & \multirow{2}{*}{VTAB} & \multirow{2}{*}{Retrieval} & Average over \\%  & Average\\
& &  & dist. shifts &  & & 38 datasets \\\midrule% & 
\cellcolor{white} & No filtering	 & 0.102 & 0.093 & 0.204 & 0.125 & 0.194 \\
\cellcolor{white} & Random subset(75\%)	 & 0.078 & 0.072 & 0.182 & 0.109 & 0.177 \\
\cellcolor{white} & Random subset(50\%)	 & 0.045 & 0.049 & 0.161 & 0.096 & 0.149 \\
\cellcolor{white} & Random subset(25\%)	 & 0.023 & 0.029 & 0.134 & 0.071 & 0.119 \\
\cellcolor{white} & Random subset(10\%)	 & 0.010 & 0.018 & 0.119 & 0.067 & 0.101 \\
\cellcolor{white} & Random subset(1\%)	 & 0.002 & 0.006 & 0.097 & 0.055 & 0.082 \\
\cellcolor{white} & Caption length	 & 0.085 & 0.080 & 0.198 & 0.116 & 0.183 \\
\cellcolor{white} & Image size	 & 0.066 & 0.064 & 0.153 & 0.101 & 0.157  \\
\cellcolor{white} & English (fasttext)	 & 0.068 & 0.068 & 0.172 & 0.095 & 0.158 \\
\cellcolor{white} & English (fasttext) and caption length	 & 0.066 & 0.065 & 0.182 & 0.095 & 0.162 \\
\cellcolor{white} & English (fasttext), caption length, and image size	& 0.045 & 0.048 & 0.164 & 0.084  & 0.148 \\
\cellcolor{white} & CLIP B32 score top 10\%	 & 0.035 & 0.046 & 0.162 & 0.072 & 0.139 \\
\cellcolor{white} & CLIP B32 score top 20\%	 & 0.076 & 0.076 & 0.182 & 0.088 & 0.171 \\
\cellcolor{white} & CLIP B32 score top 30\%	 & 0.096 & 0.090 & 0.221 & 0.104 & 0.204 \\
\cellcolor{white} & CLIP B32 score top 40\%	 & 0.081 & 0.077 & 0.200 & 0.107 & 0.191 \\
\cellcolor{white} & CLIP B32 score top 50\%	 & 0.106 & 0.097 & 0.211 & 0.113 & 0.203 \\
\cellcolor{white} & CLIP B32 score top 75\%	 & 0.103 & 0.096 & 0.210 & 0.126 & 0.196 \\
\cellcolor{white} & CLIP B32 score top 90\%	 & 0.105 & 0.096 & 0.212 & 0.127 & 0.200  \\
\cellcolor{white} & CLIP B32 threshold at 0.3 + English filter	 & 0.029 & 0.036 & 0.152 & 0.071 & 0.133  \\
\cellcolor{white} & CLIP B32 threshold at 0.28 + English filter	 & 0.035 & 0.041 & 0.168 & 0.080 & 0.145 \\
\cellcolor{white} & CLIP B32 threshold at 0.3	 & 0.076 & 0.078 & 0.199 & 0.089 & 0.181 \\
\cellcolor{white} & CLIP L14 score top 10\%	 & 0.026 & 0.037 & 0.130 & 0.069 & 0.123 \\
\cellcolor{white} & CLIP L14 score top 20\%	 & 0.060 & 0.064 & 0.161 & 0.085 & 0.152 \\
\cellcolor{white} & CLIP L14 score top 30\%	 & 0.088 & 0.087 & 0.199 & 0.098 & 0.187  \\
\cellcolor{white} & CLIP L14 score top 40\%	 & 0.100 & 0.096 & 0.217 & 0.103 & 0.206 \\
\cellcolor{white} & CLIP L14 score top 50\%	 & 0.104 & 0.098 & 0.212 & 0.114 & 0.201 \\
\cellcolor{white} & CLIP L14 score top 75\%	 & 0.103 & 0.095 & 0.189 & 0.121 & 0.190 \\
\cellcolor{white} & CLIP L14 score top 90\%	 & 0.105 & 0.095 & 0.203 & 0.123 & 0.196 \\
\cellcolor{white} & Image-based clustering (ImageNet1k)	 & 0.053 & 0.053 & 0.162 & 0.082 & 0.145 \\
\cellcolor{white} & Image-based clustering (ImageNet21k)	 & 0.063 & 0.059 & 0.173 & 0.094 & 0.166 \\
\cellcolor{white} & Text-based clustering (ImageNet1k)	 & 0.012 & 0.018 & 0.120 & 0.060 & 0.104 \\
\cellcolor{white} & Text-based clustering (ImageNet21k)	 & 0.060 & 0.064 & 0.170 & 0.090 & 0.159 \\
\cellcolor{white} & Intersect IN1k image clustering and CLIP B32 score top 30\%	 & 0.058 & 0.059 & 0.179 & 0.089 & 0.160  \\
\cellcolor{white} & Intersect IN1k image clustering and CLIP L14 score top 30\%	 & 0.049 & 0.051 & 0.171 & 0.083 & 0.149 \\
\cellcolor{white} & Intersect IN21k image clustering and CLIP B32 score top 30\%	 & 0.071 & 0.070 & 0.192 & 0.092 & 0.174 \\
\cellcolor{white} \multirow{-36}{*}{{\small \texttt{small}}}& Intersect IN21k image clustering and CLIP L14 score top 30\%	 & 0.064 & 0.065 & 0.200 & 0.085 & 0.172 \\
\midrule
\cellcolor{white} & No filtering	 & 0.370 & 0.304 & 0.387 & 0.259 & 0.376 \\
\cellcolor{white} & English (fasttext), caption length, and image size	 & 0.317 & 0.269 & 0.324 & 0.194 & 0.328 \\
\cellcolor{white} & CLIP B32 score top 30\%	 & 0.436 & 0.351 & 0.433 & 0.245 & 0.422 \\
\cellcolor{white} & CLIP B32 score top 40\%	 & 0.434 & 0.353 & 0.448 & 0.263 & 0.434  \\
\cellcolor{white} & CLIP B32 score top 50\%	 & 0.426 & 0.352 & 0.439 & 0.273 & 0.425 \\
\cellcolor{white} & CLIP B32 score top 75\%	 & 0.398 & 0.325 & 0.396 & 0.271 & 0.402 \\
\cellcolor{white} & Image-based clustering (ImageNet1k)	 & 0.363 & 0.294 & 0.347 & 0.197 & 0.341 \\
\cellcolor{white} & Image-based clustering (ImageNet21k)	 & 0.374 & 0.303 & 0.372 & 0.224 & 0.364 \\
\cellcolor{white} & Intersect IN1k image clustering and CLIP B32 score top 30\%	 & 0.415 & 0.330 & 0.413 & 0.218 & 0.396 \\
\cellcolor{white} \multirow{-10}{*}{{\small \texttt{medium}}}& Intersect IN1k image clustering and CLIP L14 score top 30\%	 & 0.405 & 0.325 & 0.399 & 0.206 & 0.380 \\
\bottomrule
\end{tabular}}
\label{tab:app_steps}
\vspace{3pt}
\end{table*}


\FloatBarrier

\section{Training details}
\label{app:train}

The full set of hyperparameters used for each scale is shown in Table \ref{tab:train-hparams}.
For choosing hyperparameters, we follow the OpenCLIP library~\cite{ilharco2021openclip}, an open source reproduction of OpenAI's CLIP. For the \texttt{small}, \texttt{medium}, and \texttt{large} tracks, these hyperparameters are equal to those in the CLIP paper, except with reduced batch size so that training runs on reasonable hardware. For the \texttt{xlarge} track, batch size is increased from that in OpenAI's CLIP to accelerate training by allowing the use of many GPUs simultaneously with high utilization. For this run we also double the learning rate following prior work~\cite{cherti2022reproducible}.

% % GMACs estimates from https://github.com/LAION-AI/CLIP_benchmark/blob/main/probe_benchmark/clip_table_2.csv
\begin{table*}
\caption{Experimental configuration for each scale, including the size of the pool we provide, the model architecture and hyperparameters.}
\setlength\tabcolsep{4.5pt}
    \renewcommand{\arraystretch}{1.1}
        \rowcolors{2}{white}{light-light-gray}
\small
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
Scale   & Model    & Train compute (MACs) & Pool size &  \# samples seen & Learning rate & AdamW $\beta_2$ & Warmup & Batch size \\\midrule
{\small \texttt{small}}    & ViT-B/32 & $9.5\times 10^{16}$ & 12.8M & 12.8M & 5e-4 & 0.98 &  500 & 4096 \\
{\small \texttt{medium}}   & ViT-B/32 & $9.5\times 10^{17}$ & 128M  & 128M  & 5e-4 & 0.98 & 500 & 4096 \\
{\small \texttt{large}}	& ViT-B/16 & $2.6\times 10^{19}$ & 1.28B & 1.28B & 5e-4 & 0.98 & 500 & 8192 \\
{\small \texttt{xlarge}}	& ViT-L/14 & $1.1\times 10^{21}$ & 12.8B  & 12.8B & 1e-3 & 0.95 & 10k & 90112 \\
\bottomrule
\end{tabular}
}
\label{tab:train-hparams}
\end{table*}



\section{Evaluation details}
\label{sec:app-eval}

Models are evaluated over a wide range of 38 tasks to measure proficiency in various domains. We include 22 of the 27 classification tasks in the test suite of \citet{radford2021learning}, excluding the few datasets that have license restrictions, are in video format, or are no longer available in their original form. We include 6 datasets that were designed to test generalization of models trained on ImageNet. We also include a majority of the Visual Task Adaptation Benchmark, excluding 3 datasets that are ill-suited for zero-shot evaluation \cite{vtab}. We include 3 datasets from the WILDS benchmark, which tests robustness to distribution shifts and spurious correlations \cite{wilds2021,sagawa2022extending}. Finally, we include 2 additional datasets, Dollar Street and GeoDE, which test robustness of classification performance across income levels and geographical regions \cite{rojas2022dollar,ramaswamy2022geode}. Furthermore, we evaluate zero-shot image and text retrieval on the Flickr30k and MSCOCO datasets, and image association on the WinoGAViL dataset \cite{flickr30k,mscoco,bitton2022winogavil}.
The complete list of evaluation tasks is given in Table~\ref{tab:eval-sets}. We show a sample from each dataset in Figure \ref{fig:downstream-samples}.


\begin{table*}
\caption{Evaluation tasks.}
\setlength\tabcolsep{4.5pt}
    \renewcommand{\arraystretch}{1.1}
        \rowcolors{2}{white}{light-light-gray}
\resizebox{\textwidth}{!}{
\centering
\begin{tabular}{lllrrrc}
\toprule
Task type & Dataset & Task & Test set size & Number of classes & Main metric & Clean\\ \midrule
\cellcolor{white}  & Caltech-101 \cite{caltech101} & Object recognition & 6,085 & 102 & mean per class & \checkmark\\
\cellcolor{white} & CIFAR-10 \cite{cifar10andcifar100} & Visual recognition & 10,000 & 10  & accuracy & \checkmark\\
\cellcolor{white} & CIFAR-100 \cite{cifar10andcifar100} & Visual recognition & 10,000 & 100 & accuracy & \checkmark\\
\cellcolor{white} & CLEVR Counts \cite{clevr,vtab} & Counting & 15,000 & 8 & accuracy & \\
\cellcolor{white} & CLEVR Distance \cite{clevr,vtab} & Distance prediction & 15,000 & 6 & accuracy & \\
\cellcolor{white} & Country211 \cite{radford2021learning,yfcc100m} & Geolocation & 21,100 & 211 & accuracy & \checkmark\\
\cellcolor{white} & DTD \cite{dtd} & Texture classification & 1,880 & 47 & accuracy & \checkmark\\
\cellcolor{white} & EuroSAT \cite{eurosat,vtab} & Satellite imagery recognition & 5,400 & 10 & accuracy & \checkmark\\
\cellcolor{white} & FGVC Aircraft \cite{fgvc_aicraft} & Aircraft recognition & 3,333 & 100 & mean per class & \checkmark\\
\cellcolor{white} & Food-101 \cite{food101} & Food recognition & 25,250 & 101 & accuracy & \checkmark\\
\cellcolor{white} & GTSRB \cite{gtsrb} & Traffic sign recognition & 12,630 & 43 & accuracy & \checkmark\\
\cellcolor{white} & ImageNet 1k \cite{deng2009imagenet} & Visual recognition & 50,000 & 1,000 & accuracy & \checkmark\\
\cellcolor{white} & ImageNet Sketch \cite{imagenetsketch} & Visual recognition & 50,889 & 1,000  & accuracy & \checkmark\\
\cellcolor{white} & ImageNet V2 \cite{imagenetv2} & Visual recognition & 10,000 & 1,000  & accuracy & \checkmark\\
\cellcolor{white} & ImageNet-A \cite{imageneta_and_imageneto} & Visual recognition & 7,500 & 200 & accuracy  & \checkmark\\
\cellcolor{white} & ImageNet-O \cite{imageneta_and_imageneto} & Visual recognition & 2,000 & 200  & accuracy & \checkmark\\
\cellcolor{white} & ImageNet-R \cite{imagenetr} & Visual recognition & 30,000 & 200  & accuracy & \checkmark\\
\cellcolor{white} & KITTI distance \cite{kitti,vtab} & Distance prediction & 711 & 4  & accuracy\\
\cellcolor{white} & MNIST \cite{lecun1998mnist} & Digit recognition & 10,000 & 10 & accuracy  & \checkmark\\
\cellcolor{white} & ObjectNet \cite{objectnet} & Visual recognition & 18,574 & 113  & accuracy & \checkmark\\
\cellcolor{white} & Oxford Flowers-102 \cite{flowers102} & Flower recognition & 6,149 & 102 & mean per class & \checkmark\\
\cellcolor{white} & Oxford-IIIT Pet \cite{pets,vtab} & Pet classification & 3,669 & 37 & mean per class & \checkmark\\
\cellcolor{white} & Pascal VOC 2007 \cite{pascal-voc-2007} & Object recognition & 14,976 & 20 & accuracy & \checkmark\\
\cellcolor{white} & PatchCamelyon \cite{patchcamelyon,vtab} & Metastatic tissue cls. & 32,768 & 2  & accuracy\\
\cellcolor{white} & Rendered SST2 \cite{vtab} & Sentiment classification & 1,821 & 2  & accuracy & \checkmark\\
\cellcolor{white} & RESISC45 \cite{resisc45,vtab} & Satellite imagery recognition & 6,300 & 45 & accuracy  & \checkmark\\
\cellcolor{white} & Stanford Cars \cite{cars} & Vehicle recognition & 8,041 & 196 & accuracy  & \checkmark\\
\cellcolor{white} & STL-10 \cite{stl10} & Visual recognition & 8,000 & 10  & accuracy & \checkmark\\
\cellcolor{white} & SUN-397 \cite{sun397} & Scene recognition & 108,754 & 397 & accuracy & \checkmark \\
\cellcolor{white} & SVHN \cite{svhn,vtab} & Digit recognition & 26032 & 10 & accuracy & \checkmark \\
\cellcolor{white} & iWildCam \cite{beery2020iwildcam,wilds2021} & Animal recognition & 42,791 & 182 & macro F1 score & \checkmark \\
\cellcolor{white} & Camelyon17 \cite{bandi2018detection,wilds2021} & Metastatic tissue cls. & 85,054 & 2 & accuracy \\
\cellcolor{white} & FMoW \cite{christie2018functional,wilds2021} & Satellite imagery recognition & 22,108 & 62 & worst-region acc.  & \checkmark\\
\cellcolor{white} & Dollar Street \cite{rojas2022dollar} & Object recognition & 3,503 & 58 & worst-income top-5 acc.  & \checkmark\\
\cellcolor{white} \multirow{-35}{*}{Classification}& GeoDE \cite{ramaswamy2022geode} & Object recognition & 12,488 & 40 & worst-region acc. & \checkmark \\
\midrule
 \cellcolor{white} & Flickr30k \cite{flickr30k} & Image and text retrieval & 31,014 & N/A & R@1 & \checkmark \\
\cellcolor{white} & MSCOCO \cite{mscoco} & Image and text retrieval & 5,000 & N/A & R@1  & \checkmark\\
\cellcolor{white} \multirow{-3}{*}{Retrieval}& WinoGAViL \cite{bitton2022winogavil} & Commonsense association & 3,563 & N/A & Jaccard score & \checkmark \\
\bottomrule
\end{tabular}} 
\label{tab:eval-sets}
\end{table*}


\begin{figure*}
    \centering
    \includegraphics[width=.7\textwidth]{figures/downstream_samples.pdf}
    \caption{Randomly sampled images from the evaluation datasets we consider.}
    \label{fig:downstream-samples}
\end{figure*}




\paragraph{Prompt choice.} Since we perform zero-shot evaluation, prompt and class name selection is important, and can have a significant impact on the results. To avoid heavy prompt engineering and overtuning to individual models, we opt to use the prompt templates used in \citet{radford2021learning} whenever possible. Most datasets come with pre-defined class names, but some are overwritten with more descriptive labels, again based on previous literature. For datasets with no precedent in zero-shot evaluation, we reuse prompt templates from other datasets with a similar domain and task (e.g., SVHN is evaluated with MNIST prompts and class names).

\paragraph{Evaluation metrics.} For the majority of classification tasks, the primary evaluation metric is accuracy. For certain datasets with class imbalances, we instead compute mean per-class accuracy, as done in \citet{radford2021learning}. On the WILDS benchmark datasets, we use the primary metric specified for each dataset on their leaderboard. Dollar Street and GeoDE test model generalization across socioeconomic and geographic diversity. Thus, for Dollar Street, we compute worst-group top-5 accuracy, with groups defined by income level, emulating \citet{rojas2022dollar}; for GeoDE, we compute worst-group accuracy, with groups defined by region (Africa, Americas, West Asia, East Asia, Southeast Asia, and Europe), as defined in \citet{ramaswamy2022geode}. For the image-text retrieval tasks, Flickr and MSCOCO, we compute both image and text recall (fraction of text captions for which the correct image was selected and vice versa), and plot their arithmetic mean. On WinoGAViL, we compute the Jaccard score (intersection-over-union) for each example, and show results for the harder samples (10 and 12 candidates). More information on WinoGAViL evaluation can be found in \citet{bitton2022winogavil}.

\paragraph{Clean subset.} For five of our evaluation tasks (the two CLEVR tasks, the two Camelyon tasks, and KITTI) the zero-shot performance of all evaluated models appears to be close to that of random guessing, and lack correlation to the type of filtering method used (see \Cref{fig:imagenet-vs-all-breakdown}). Consequently, we studied performance averaged only on the remaining 33 tasks, but found not substantial qualitative differences in our results.  As a result, we opted to report the average on the full evaluation suite throughout our study. 

\begin{figure*}
    \centering
    \includegraphics[width=.9\textwidth]{figures/linear_probes.pdf}
    \caption{Zero-shot ImageNet and Linear probe ImageNet performance for models from Tables \ref{tab:main} and \ref{tab:byod}. Relative ordering of models demonstrates high rank correlations of 0.99 and 1.0 for \pool and \byod respectively.}
    \label{fig:linear-probes}
\end{figure*}

\paragraph{Zero-shot vs. fine-tuning protocols.}
One critical decision in \datanet is how exactly to evaluate models and whether or not to fine-tune models on evaluation tasks (i.e., supervised fine-tuning directly on task training sets). We opt for zero-shot evaluation, where a models are applied to downstream tasks directly to 1) ease computational burden on \users and 2) measure the out-of-the-box generalization capabilities of our models. To validate this design decision, we conduct linear probes on all models presented in Tables \ref{tab:main} and \ref{tab:byod} on ImageNet. We follow a standard probing protocol and fine-tune the last linear layer from zero-shot initialization for 40 epochs with learning rate 1e-3, batch size 256, AdamW optimizer with default settings with the exception of weight decay (that we set to zero), and a cosine annealing schedule. As seen in Figure \ref{fig:linear-probes}, zero-shot and linear probe performance follow similar trends for both filtering and \byod tracks. Moreover the Spearman rank correlation between the two protocols over the models considered is 0.99 for the filtering track and 1.0 for \byod. This suggests that better zero-shot models on ImageNet are correlated with better representations of linear probe fine-tuning on ImageNet.

\section{Baseline details}
\label{sec:app-baselines}
Here we provide additional details on the creation of our baseline subsets.
To highlight the qualitative differences between the filtering strategies we also provide visualization for \emph{No filtering} (Figure \ref{fig:no_filter}), \emph{Basic filtering} (Figure \ref{fig:basic_filter}), and \emph{CLIP score (L/14 30\%)} (Figure \ref{fig:clip_filter}), which can all be found in Table~\ref{tab:main}. Notice that No filtering gives relatively noisy data (e.g., matching a bicycle with a caption: ``IMG\_2187.jpg''), while CLIP score samples give qualitatively more descriptive cations.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/appx-no_filter.pdf}
    \caption{An i.i.d. sample from {\small \texttt{small}} \pool generated after applying the \emph{No filter} strategy. Hence, these samples represent random images from \pool.}
    \label{fig:no_filter}
    % \vspace{-15mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/appx-basic.pdf}
    \caption{An i.i.d. sample from {\small \texttt{small}} \pool generated after applying the \emph{Basic filter} strategy.}
    \label{fig:basic_filter}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/appx-clip_filter.pdf}
    \caption{An i.i.d. sample from {\small \texttt{small}} \pool generated after applying the CLIP score (L/14 30\%)} strategy.
    \label{fig:clip_filter}
\end{figure*}


\FloatBarrier

\subsection{Filtering track}\label{sec:app-baselines-pool}

\paragraph{Basic filtering.} 

For language detection, we use Fasttext 0.92, version lid.176, and cld3 - library gcld3 3.0.13.
We count the number of words in each caption by splitting using whitespaces.


\paragraph{CLIP thresholds.}
We use OpenAI pretrained CLIP ViT-B/32 and ViT-L/14 models~\citep{radford2021learning} to compute the cosine similarity text and image tower outputs as the CLIP scores. On the {\small \texttt{small}} and {\small \texttt{medium}} pools, we also experiment with baselines that filter out samples in the top few percentiles of CLIP scores. Specifically, we try baselines that use samples with top \{1,2,5\}-30\% CLIP scores (ViT-B/32 model), and the performance is sightly better on the {\small \texttt{small}} pool (at most 0.5 gain of averaged accuracy) while slightly worse on the {\small \texttt{medium}} pool (0.4-0.8 loss of averaged accuracy). In Table \ref{tab:filtering_thresholds}, we show how the CLIP score thresholds relate to the fraction of the pool retained by the filter.

\begin{table}
    \renewcommand{\arraystretch}{1.1}
        \rowcolors{2}{white}{light-light-gray}
\caption{CLIP threshold filtering configurations. ``Fraction'' denotes the size of the filtered subset relative to the pool.}
\setlength\tabcolsep{2pt}
% \renewcommand{\arraystretch}{0.9}
\footnotesize
\centering
\begin{tabular}{lccc}
\toprule
CLIP model   & En. filtering   & Threshold & Fraction \\
\midrule
ViT-B/32 & \xmark & 0.384 & 1\% \\
ViT-B/32 & \xmark & 0.358 & 3\% \\
ViT-B/32 & \cmark & 0.300 & 10.2\% \\
ViT-B/32 & \xmark & 0.325 & 10\% \\
ViT-B/32 & \cmark & 0.28 & 7.4\% \\
ViT-B/32 & \xmark & 0.300 & 20\% \\
ViT-B/32 & \xmark & 0.281 & 30\% \\
ViT-B/32 & \xmark & 0.263 & 40\% \\
ViT-B/32 & \xmark & 0.247 & 50\% \\
ViT-B/32 & \xmark & 0.215 & 75\% \\
ViT-B/32 & \xmark & 0.193 & 90\% \\
\midrule
ViT-L/14 & \xmark & 0.364 & 1\% \\
ViT-L/14 & \xmark & 0.334 & 3\% \\
ViT-L/14 & \cmark & 0.300 & 5.4\% \\
ViT-L/14 & \xmark & 0.295 & 10\% \\
ViT-L/14 & \cmark & 0.280 & 3.3\% \\
ViT-L/14 & \xmark & 0.266 & 20\% \\
ViT-L/14 & \xmark & 0.243 & 30\% \\
ViT-L/14 & \xmark & 0.222 & 40\% \\
ViT-L/14 & \xmark & 0.203 & 50\% \\
ViT-L/14 & \xmark & 0.160 & 75\% \\
ViT-L/14 & \xmark & 0.129 & 90\% \\
\bottomrule
\end{tabular}
\label{tab:filtering_thresholds}
\end{table}


\paragraph{Text-based filtering.} 
Each synset is represented by a synset offset that can be used to retrieve the synset from WordNet. In order to verify if a caption has a word corresponding to a synset from our set we iterate over every word and retrieve the synsets that this word can describe (using nltk.corpus WordNet). Following that, we retrieve the most likely lemma representing that synset, find its synset offset, and check if the number is part of the IN21K or IN1K sets.\footnote{For the ImageNet 21K synsets, we have used the list in \url{https://storage.googleapis.com/bit_models/imagenet21k_wordnet_ids.txt}} 

\paragraph{Text-based sampling.} 
This baseline uses text only to filter labels which mention concepts (synsets) appearing in IN21K, and applies a temperature parameter to control how equally-represented different concepts are in the dataset.
For synset $j$, let $N_j$ be the number of examples containing words matched to that synset, where as before for each word we only match the most likely synset. Furthermore, for image-text pair $i$ let $T_i$ be the set of synset matched to the caption. 

The probability of sampling example $i$ is proportional to either 
$
\frac{1}{|T_i|} \sum_{j \in T_{i}} N_{j}^{\alpha-1}
$ (average synset score in the data point)
or 
$
\max_{j \in T_{i}} N_{j}^{\alpha-1}
$ (maximum synset score in the data point), where $\alpha$ is a ``temperature'' parameter controlling the flatness of the distribution. We sample examples with replacement but discard any example repeated more than 100 times.

\paragraph{Image-based filtering.}
We now provide a detailed description of the Image-based filtering procedure. First, since the core of the procedure concerns only image content, we begin with basic text-bsaed filtering: we remove from the pool only all examples with non-English captions (as determined by fasttext), and all examples whose captions have less than two words or less than six characters. 

Next, we use clustering of image embeddings to select a subset of examples whose image content is related to a clean training set of interest.
Let $e_1, \ldots, e_M$ denote the CLIP image embeddings of the remaining examples in the pool. We cluster these embeddings into $K=10^5$ clusters using Faiss with 20 iterations, and let $c_1, \ldots, c_K$ denote the resulting cluster centers. Due to memory constraints, for the \texttt{large}  and \texttt{xlarge} pools,
we perform the clustering on a random subset of about 160M examples (that pass the basic text-based filtering). For an embedding vector $v$, let
\[
I(v) = \arg\max_{i\le K} 
{\langle v , c_i \rangle}
\]
 denote the index of the cluster center nearest to $v$ as measured by inner product.
 Let $f_1, \ldots, f_N$ denote the CLIP image embeddings of a clean supervised training set (we experiment with either ImageNet 1K or ImageNet 21K), and let
\[
\mathcal{S} = \{I(f_i) \mid 1 \le i \le N\}
\]
be the set of cluster indices who are nearest neighbors to some clean training set image. We then keep only images in the pool whose nearest cluster center is in $\mathcal{S}$. That is, out of the $M$ examples passing the text-based filtering, the output subset keeps the examples with indices
\[
\{1\le j\le M\mid I(e_j)\in\mathcal{S}\}.
\]


\paragraph{Image-based sampling.}
In addition to filtering methods, we experiment with cluster-based sampling methods. First, we compute the score of $i$-th cluster $s_i$ as the number of ImageNet data assigned to this cluster. Then, for parameter $\alpha>0$ we define a distribution over the pool by sampling cluster $i$ with probability $\frac{s_i^\alpha}{\sum_{j} s_j^\alpha}$ and uniformly sampling an example for the cluster, rejecting any example repeated more than 100 times. We try 5 different $\alpha$, i.e., $\{0, 0.2, 0.5, 1.0, 2.0\}$, and the best average accuracy is obtained when $\alpha=0.2$, while the performance is still worse than the image-based filtering on the \texttt{small} and \texttt{medium} pool. We therefore do not include this line of baselines in the experiments of \texttt{large} pool.


\paragraph{ImageNet distance filtering.}
We rank the samples in the pool by the minimum embedding distance (1 minus cosine similarity) between its image and the ImageNet images; both embeddings are obtained from OpenAI pretrained CLIP ViT-L/14 model~\citep{radford2021learning}. Then we select top images by different fractions as in image-based filtering methods.

\subsection{\byod track}
\label{app:byod}

We experiment with the following data sources:

\begin{itemize}[leftmargin=6pt,topsep=0pt,itemsep=0pt,parsep=4pt]
\item CC12M \cite{changpinyo2021conceptual}: images and HTML alt-text crawled and filtered from web pages.
\item YFCC15M: this is the 15M subset of the YFCC100M dataset \cite{yfcc100m} that \citet{radford2021learning} used for dataset ablation in their CLIP paper.
\item RedCaps \cite{desai2021redcaps}: 12M images and corresponding captions were crawled from 350 manually curated subreddits between 2008 and 2020.
\item Shutterstock: 106M images and captions were obtained from the Shutterstock website in 2021 \cite{nguyen2022quality}. We use the ``photos'' subset of this dataset, with 58M samples, which we found performed best, unless specified otherwise.
\item WIT \cite{srinivasan2021wit}:  Image-text pairs from Wikipedia pages. We use the attribution fields as captions, which we found performed best.
\item COYO \cite{coyo700m}: A collection of 700M image-text pairs from Common Crawl.
\item LAION-2B \cite{laion5b}: A 2.32 billion english subset of LAION-5B.
\item LAION-COCO: A dataset with 600M images from LAION-5B and synthetic captions.\footnote{\url{https://laion.ai/blog/laion-coco/}}
\item LAION-A: According to \href{https://laion.ai/}{laion.ai}, LAION-A is a 900M subset of LAION-2B \cite{laion5b} with the aesthetic filtering procedure used in LAION-aesthetic\footnote{\url{https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md}} and pHash deduplication \cite{idealods2019imagededup}.
\end{itemize}


\begin{table*}
\caption{Measuring the quality of external data sources}
    \renewcommand{\arraystretch}{1.1}
        \rowcolors{3}{light-light-gray}{white}
\setlength\tabcolsep{4pt}
\small
\centering
\resizebox{\textwidth}{!}{

\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Dataset size} & \multirow{2}{*}{ImageNet acc.} & Avg. accuracy & \multirow{2}{*}{Avg. cos. sim. (B/32)} & \multirow{2}{*}{Avg. cos. sim. (L/14)} \\
  &  &  & ImageNet and OOD sets & &  \\
\midrule
CC12M & 10M & 27.8 & 34.0 & 0.306 & 0.268 \\
YFCC15M & 15M & 22.6 & 24.6 & 0.262 & 0.198 \\
RedCaps & 11M & 26.8 & 31.5 & 0.281 & 0.240 \\
Shutterstock & 15M & 21.0 & 28.3 & 0.314 & 0.273 \\
\bottomrule
\end{tabular}
}
\label{tab:app_byod_quality}
\end{table*}

In Table \ref{tab:app_byod_quality}, we use some heuristics to measure the quality of the external data sources. First, following \citet{nguyen2022quality}, we train a CLIP model on a 5M random subset from each source, and evaluate the performance of the resulting models on ImageNet and ImageNet-derived distributions --- ImageNet-V2 \cite{imagenetv2}, ImageNet-R \cite{imagenetr}, ImageNet-Sketch \cite{imagenetsketch} and ObjectNet \cite{objectnet}. Moreover, for each data source, we use OpenAI's pretrained CLIP ViT-B/32 and ViT-L/14 models to compute the cosine similarity between image and text embeddings of a data point, and obtain the average cosine similarity score for the whole dataset.

\begin{table*}

\setlength\tabcolsep{4pt}
\renewcommand{\arraystretch}{1.1}
        \rowcolors{3}{light-light-gray}{white}
\small
\centering
\caption{Zero-shot performance for select baselines in the \byod track. Unless specified otherwise, \pool means our pool filtered with CLIP score (L/14, 30\%). }
\resizebox{\textwidth}{!}{
\begin{tabular}{lclccccc}
\toprule
\multirow{2}{*}{Scale} & \multirow{2}{*}{Data source}   & Training & \multirow{2}{*}{ImageNet} & ImageNet & \multirow{2}{*}{VTAB} & \multirow{2}{*}{Retrieval} & Average over \\%  & Average\\
& & dataset size &  & dist. shifts &  & & 38 datasets \\\midrule% & 

 \cellcolor{white} & \#0 & CC12M & 0.099 & 0.080 & 0.223 & 0.160 & 0.202 \\% & 0.169 \\
 \cellcolor{white} & \#1 & LAION15M & 0.083 & 0.076 & 0.210 & 0.119 & 0.187 \\% & 0.158\\
 \cellcolor{white} & \#2 & RedCaps & 0.076 & 0.066 & 0.177 & 0.127 & 0.167 \\% & 0.145\\
 \cellcolor{white} & \#3 & Shutterstock 15M & 0.083 & 0.070 & 0.214 & 0.128 & 0.183 \\% & 0.159\\
 \cellcolor{white} & \#4 & YFCC15M & 0.071 & 0.046 & 0.182 & 0.120 & 0.162 \\% & 0.133\\
\cellcolor{white}  & \#5 & \#0 + \#1 + \#2 & 0.097 & 0.084 & 0.208 & 0.131 & 0.192 \\% & 0.172\\
\cellcolor{white}  & \#6 & \#0 + \#1 + \#3 & 0.091	 & 0.081 & 0.222 & 0.138 & 0.202\\%  & 0.172\\
\cellcolor{white}  & \#7 & \#0 + \#2 + \#3 + \#4 & 0.095 & 0.075 & 0.205 & 0.135 & 0.184 \\% & 0.160\\
\cellcolor{white}  \multirow{-9}{*}{{\small \texttt{small}}}& \#8 & \#0--4 & 0.093	 & 0.076 & 0.205 & 0.135 & 0.191 \\% & 0.163\\
\midrule
\cellcolor{white}  & \#9 & CC12M & 0.245 & 0.189 & 0.283 & 0.206 & 0.266\\%  & 0.257 \\
\cellcolor{white}  & \#10 & LAION15M & 0.270 & 0.215 & 0.317 & 0.181 & 0.300 \\% & 0.296\\
\cellcolor{white}  & \#11 & RedCaps & 0.237 & 0.166 & 0.271 & 0.150 & 0.261 \\% & 0.257\\
\cellcolor{white}  & \#12 & Shutterstock 15M & 0.229 & 0.191 & 0.316 & 0.190 & 0.284 \\% & 0.273\\
\cellcolor{white}  & \#13 & YFCC15M & 0.232 & 0.137 & 0.263 & 0.174 & 0.251 \\% & 0.239\\
\cellcolor{white}  & \#14 & \#9 + \#10 + \#11 & 0.376 & 0.287 & 0.387 & 0.227 & 0.358 \\% & 0.357\\
\cellcolor{white}  & \#15 & \#9 + \#10 + \#12 & 0.342 & 0.278 & 0.362 & 0.242 & 0.349 \\% & 0.351\\
\cellcolor{white}  & \#16 & \#9 + \#11 + \#12 + \#13 & 0.360 & 0.268 & 0.365 & 0.190 & 0.338 \\% & 0.340\\
\cellcolor{white}  & \#17 & \#9--13 & 0.371 & 0.285 & 0.408 & 0.194 & 0.361 \\% & 0.357\\
\cellcolor{white}  & \#18 & Shutterstock illustration & 0.053 & 0.094 & 0.205 & 0.112 & 0.179 \\% & 0.156\\
\cellcolor{white}  & \#19 & Shutterstock photo & 0.342 & 0.209 & 0.364 & 0.248 & 0.323 \\% & 0.324\\
\cellcolor{white}  & \#20 & Shutterstock vectors & 0.072 & 0.151 & 0.216 & 0.129 & 0.206 \\% & 0.185\\
\cellcolor{white}  & \#21 & Shutterstock full & 0.313 & 0.254 & 0.353 & 0.240 & 0.335 \\% & 0.338\\
\cellcolor{white}  & \#22 & WIT full & 0.096 & 0.063 & 0.196 & 0.088 & 0.175 \\% & 0.143\\
\cellcolor{white}  & \#23 & WIT English & 0.051 & 0.038 & 0.145 & 0.073 & 0.142 \\% & 0.112 \\
\cellcolor{white}  & \#24 & COYO & 0.272 & 0.235 & 0.333 & 0.249 & 0.314 \\% & 0.308\\
\cellcolor{white} \multirow{-17}{*}{{\small \texttt{medium}}}& \#25 & LAION-COCO & 0.209 & 0.205 & 0.293 & 0.243 & 0.288 \\\midrule% & 0.279\\\midrule
\cellcolor{white}  & \#26 &  Shutterstock illustration & 0.337 & 0.203 & 0.307 & 0.223 & 0.298 \\% & 0.299\\
\cellcolor{white}  & \#27 & Shutterstock photo & 0.485 & 0.304 & 0.432 & 0.311 & 0.389 \\% & 0.397\\
\cellcolor{white}  & \#28 & Shutterstock vectors & 0.126 & 0.223 & 0.244 & 0.152 & 0.243 \\% & 0.228\\
\cellcolor{white} & \#29 & Shutterstock full & 0.500	 & 0.412 & 0.472 & 0.335 & 0.447 \\% & 0.459 \\
\cellcolor{white} & \#30 & COYO & 0.615 & 0.504 & 0.529 & 0.332 & 0.522 \\% & 0.546 \\
\cellcolor{white} & \#31 & LAION-COCO & 0.355 & 0.351 & 0.395 & 0.366 & 0.388 \\% & 0.387\\
\cellcolor{white} & \#32 & COYO + LAION-COCO & 0.528 & 0.458 & 0.479 & 0.466 & 0.488 \\% & 0.500 \\
\cellcolor{white} & \#33 & LAION-A & 0.611 & 	0.474 & 0.501 & 0.414 & 0.495 \\% & 0.518\\
\cellcolor{white} & \#34 & \pool +  \#9--13 & 0.602 & 0.498 & 0.541 & 0.284 & 0.527 \\% & 0.546\\
\cellcolor{white} & \#35 & \pool +  \#9--13 (2x upsampled) & 0.613 & 0.507 & 0.559 & 0.293 & 0.532 \\% & 0.556\\
\cellcolor{white} & \#36 & \pool +  \#9--13 (4x upsampled) & 0.615 & 0.514 & 0.553 & 0.295  & 0.533 \\% & 0.551\\
\cellcolor{white} & \#37 & \pool +  \#9--13 (6x upsampled) & 0.620 & 0.519 & 0.558 & 0.301 & 0.538 \\% & 0.556\\
\cellcolor{white} & \#38 & \pool +  \#9--13 (8x upsampled) & 0.624 & 0.520 & 0.533& 0.302 & 0.526 \\% & 0.554\\
\cellcolor{white} & \#39 & \pool +  \#9--13 (10x upsampled) & 0.621 & 0.520 & 0.540 & 0.303 & 0.527 \\% & 0.552\\
\cellcolor{white} & \#40 & \pool +  COYO & 0.561 & 0.472 & 0.504 & 0.375 & 0.503 \\% & 0.525\\
\cellcolor{white} & \#41 & \pool + LAION-A & 0.607 & 0.480 & 0.531 & 0.386 & 0.517 \\% & 0.540\\
\cellcolor{white} & \#42 & \pool + LAION-COCO & 0.522 & 0.457 & 0.513 & 0.374 & 0.504 \\%  & 0.522\\
\cellcolor{white} & \#43 & \pool +  \#11+\#13+\#19 & 0.609 & 0.508 & 0.546 & 0.303 & 0.525 \\% & 0.552\\
\cellcolor{white} & \#44 & \pool +  \#11+\#13+\#19 (2x upsampled) & 0.621 & 0.509 & 	0.547 & 0.315 & 0.530 \\% & 0.558\\
\cellcolor{white} & \#45 & \pool + \#11+\#13+\#19 (4x upsampled) & 0.632 & 0.515 & 0.533 & 0.316 & 0.522 \\% & 0.552\\
\cellcolor{white} & \#46 & \pool +  \#11+\#13+\#19 (6x upsampled) & 0.635 & 0.515 & 0.535 & 0.329 & 0.521 \\% & 0.543\\
\cellcolor{white} & \#47 & \pool +  \#11+\#13+\#19 (8x upsampled) & 0.633 & 0.515 & 0.523 & 0.328 & 0.520 \\% & 0.544\\
\cellcolor{white} \multirow{-23}{*}{{\small \texttt{large}}}& \#48 & \pool +  \#11+\#13+\#19 (10x upsampled) & 0.630 & 0.513 & 0.523 & 0.317 & 0.510 \\\midrule% & 0.539\\\midrule
\cellcolor{white}  & \#49 & \pool +  \#11+\#13+\#19 & 0.766	& 0.660 & 0.662 & 0.394 & 0.648 \\% & 0.681\\
\cellcolor{white} & \#50 & \pool +  \#11+\#13+\#19 (6x upsampled) & 0.776 & 0.671 & 0.633 & 0.410 & 0.638 \\% & 0.681\\
\cellcolor{white} \multirow{-3}{*}{{\small \texttt{xlarge}}}& \#51 & \pool +  \#11+\#13+\#19 (18x upsampled) & 0.771 & 0.667 & 0.629 & 0.418 & 0.633 \\% & 0.672\\
\bottomrule
\end{tabular}}
\label{tab:byod-extra}
\vspace{3pt}

\end{table*}


\subsubsection{Additional results}

We present a series of additional results for the \byod track in Table \ref{tab:byod-extra}.

\section{Fairness and biases}
\label{app:fairness}

To study the biases displayed by our models, we include two diversity-related datasets, Dollar Street \cite{rojas2022dollar} and GeoDE \cite{ramaswamy2022geode}, in our evaluation suite, and perform further analysis on the face datasets FairFace \cite{karkkainen2021fairface} and UTKFace \cite{utkface} with demographic labels, following \citet{radford2021learning}.

\subsection{Diversity}


\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/robustness_diversity.pdf}
    \caption{Comparison of average and worst-group scores for Dollar Street and GeoDE diversity datasets. On Dollar Street, our overall higher-performing models display a larger worst-group performance gap (corresponding to lower income households). GeoDE does not appear to show this trend.}
    \label{fig:robustness_diversity}
\end{figure*}


We break down model performance on the Dollar Street and GeoDE datasets in Figure~\ref{fig:robustness_diversity}. Dollar Street consists of images of household items taken in homes around the world, and represents a wide socioeconomic range that includes homes with no Internet access \cite{rojas2022dollar}. The objects belong to ImageNet categories, and the task is image classification. Standard ImageNet-trained models achieve monotonically increasing performance levels with higher household income levels \cite{rojas2022dollar}. Here we use the income-based subgroups defined in \citet{rojas2022dollar}, and find a similar bias as discovered in their paper. While our trained models show a smaller worst-group performance gap than an ImageNet-trained ResNet-50, they underperform a model fine-tuned on Dollar Street. Models with higher average accuracy show a larger worst-group gap, which future work should try to address.

GeoDE consists of images of everyday items and objects, which again fall into ImageNet categories. The dataset represents six world regions equally, and primarily aims to promote geographic diversity of datasets \cite{ramaswamy2022geode}. Both ImageNet models and our models show less bias under this distribution compared to Dollar Street, with a smaller worst-group accuracy gap. The trends show that performance across all regions improves steadily with increased scale, and the performance approaches that of a model fine-tuned on GeoDE. While we know that classifiers trained specifically on ImageNet can display geographic biases \cite{ramaswamy2022geode}, these biases are not apparent in our GeoDE model evaluations. Future work is needed to investigate the extent to which our models have geographic biases not evaluated in GeoDE.


\subsection{Fairness}

Emulating \citet{radford2021learning}, we evaluate our best models from the filtering and \byod track baselines on the human face datasets FairFace and UTKFace, using zero-shot classification to predict race, gender, and age. Note that these are not intended end-goals of the model or benchmark, but rather probes into models behave differently across demographic subgroups. As described in Appendix~\ref{app:face}, our filleting track models are trained on images with faces blurred.
Nevertheless, these models still perform significantly above random chance on face classification. We hypothesize that this is due to a combination of faces bypassing our face blurring filter in the training data, contextual clues outside of the face region, or signal associated with skin color.
The BYOD track model performs even better than the Filtering track model. We hypothesize that this is because BYOD data is used off-the-shelf and hence contains non-blurred faces.
In Table~\ref{tab:app_fairness_overall}, we present overall accuracy for these three traits. Note that race is treated as a binary variable (white or non-white) to enable comparison to prior results; gender is a binary variable (male or female) according to annotations; and age is binned into 9 ranges according to the annotation precision of FairFace. The \byod model, performs better at distinguishing gender, but is worse at distinguishing race and age.

We further break down these statistics over the intersection of race and gender, examining gender classification accuracies in Table~\ref{tab:app_fairness_intersection}. We find that there are drastic differences in accuracy across different subgroups, varying by both race and gender. The filtering models shows a tendency to misclassify Black, Southeast Asian, and East Asian males as females at 20.7\%, 17\%, and 19.3\% respectively on FairFace. Furthermore, we find that while the \byod model improves accuracy, in FairFace most of this improvement is on men (ranging from 1.7pp gain to 9.9pp gain), while on women, it offers little change (ranging from 0.6pp gain to 6.2pp drop).

Following \citet{radford2021learning}, we also examined associations of particular demographics with potentially harmful language. We replicate their setup with two classification task: (1) including race-gender intersection classes (e.g. ``black woman'', ``indian man'', etc.), as well as several harmful crime-related terms (``thief'', ``criminal'', ``suspicious person'') and (2) out same race-gender intersection classes as well as non-human terms (``animal'', ``gorilla'', ``chimpanzee'', ``orangutan''). We compute the frequency of misclassification of people into one of the harmful categories and run these experiments on FairFace and UTKFace separately. The results are shown in Table~\ref{tab:app_fairness_harm}. Unlike in \citet{radford2021learning}, we find that our models have a very small probability of classifying human faces as non-human, with a max score across all subgroups of 0.1\%. However, a significant proportion of people are misclassified as criminal. The model is better at classifying race and gender, but also more susceptible to assigning unfounded associations with images. This again highlights the importance of dataset curation and the risks associated with zero-shot classification on models trained on such web-scraped datasets.


\begin{table*}

\renewcommand{\arraystretch}{1.1}
        \rowcolors{2}{white}{light-light-gray}
\caption{Overall race, gender, and age classification accuracy of our two best {\small\texttt{xlarge}} baselines, Image-based $\cap$ CLIP score (L/14 30\%) for the filtering track and \pool, CLIP score + 4 external sources (upsampled 6x) for the \byod track. Race classification was binary (white or non-white) as in \citet{karkkainen2021fairface}.}
\setlength\tabcolsep{4pt}
\small
\centering
\begin{tabular}{llccc}
\toprule
Dataset & Track & Race & Gender & Age \\
\midrule
 \cellcolor{white}& Filtering & 86.4 & 91.7 & 34.3 \\
\cellcolor{white}\multirow{-2}{*}{FairFace}& \byod & 76.5 & 93.9 & 33.8 \\
\midrule
\cellcolor{white} & Filtering & 86.2 & 93.8 & 39.5 \\
\cellcolor{white}\multirow{-2}{*}{UTKFace}& \byod & 86.1 & 95.5 & 38.6 \\
\bottomrule
\end{tabular}
\label{tab:app_fairness_overall}
\end{table*}


\begin{table*}

\renewcommand{\arraystretch}{1.1}
        \rowcolors{3}{light-light-gray}{white}
\caption{Gender classification accuracy of our two best {\small\texttt{xlarge}} baselines, Image-based $\cap$ CLIP score (L/14 30\%) for the filtering track and \pool, CLIP score + 4 external sources (upsampled 6x) for the \byod track.}
\setlength\tabcolsep{4pt}

\small
\centering
FairFace
\\
\resizebox{\textwidth}{!}{
\begin{tabular}{llccccccc}
\toprule
\multirow{2}{*}{Track} & \multirow{2}{*}{Gender} & \multicolumn{7}{c}{Race} \\
& & Black & White & Indian & Latino/Hispanic & Middle Eastern & Southeast Asian & East Asian \\
\midrule
\cellcolor{white} & Male & 79.3 & 91.3 & 90.8 & 90.4 & 95.7 & 83.0 & 80.7 \\
\cellcolor{white} \multirow{-2}{*}{Filtering}& Female & 95.4 & 96.6 & 94.2 & 96.6 & 96.5 & 97.2 & 98.2 \\
\midrule
\cellcolor{white} & Male & 89.2 & 94.8 & 93.2 & 93.4 & 97.4 & 90.2 & 90.6 \\
\cellcolor{white} \multirow{-2}{*}{\byod}& Female & 89.2 & 96.0 & 94.2 & 96.0 & 96.2 & 97.1 & 97.0 \\
\bottomrule
\end{tabular}
}
\\
\vspace{10pt}
UTKFace
\\

\renewcommand{\arraystretch}{1.1}
        \rowcolors{3}{light-light-gray}{white}
\begin{tabular}{llccccccc}
\toprule
\multirow{2}{*}{Track} & \multirow{2}{*}{Gender} & \multicolumn{5}{c}{Race} \\
& & Black & White & Indian & Asian & Other \\
\midrule
\cellcolor{white}  & Male & 95.4 & 92.5 & 91.7 & 73.1 & 84.2 \\
\cellcolor{white} \multirow{-2}{*}{Filtering}& Female & 97.3 & 98.7 & 97.4 & 98.3 & 97.4 \\
\midrule
\cellcolor{white}  & Male & 96.8 & 95.9 & 94.7 & 85.7 & 90.4 \\
\cellcolor{white} \multirow{-2}{*}{\byod}& Female & 96.3 & 97.7 & 96.8 & 95.9 & 95.6 \\
\bottomrule
\end{tabular}
\label{tab:app_fairness_intersection}
\end{table*}

\begin{table*}

\renewcommand{\arraystretch}{1.1}
        \rowcolors{3}{light-light-gray}{white}
\caption{Harmful misclassification rates of our two best {\small\texttt{xlarge}} baselines, Image-based $\cap$ CLIP score (L/14 30\%) for the filtering track and \pool, CLIP score + 4 external sources (upsampled 6x) for the \byod track. While very few samples are misclassified as non-human, the filter track model assigns a crime-related label to a significant portion of people, and this is exacerbated by the \byod model in many cases.}
\setlength\tabcolsep{4pt}
\renewcommand{\arraystretch}{0.9}
\small
\centering
FairFace
\\
\resizebox{\textwidth}{!}{

\begin{tabular}{llccccccc}
\toprule
\multirow{2}{*}{Track} & & \multicolumn{7}{c}{Race} \\
& & Black & White & Indian & Latino/Hispanic & Middle Eastern & Southeast Asian & East Asian \\
\midrule
\cellcolor{white}& Crime-related & 4.4 & 24.3 & 8.8 & 14.3 & 23.7 & 7.4 & 8.6 \\
\cellcolor{white}\multirow{-2}{*}{Filtering} & Non-human & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\midrule
 \cellcolor{white}& Crime-related & 18.4 & 16.8 & 21.5 & 22.9 & 20.9 & 35.3 & 30.9 \\
\cellcolor{white}\multirow{-2}{*}{\byod}& Non-human & 0.0 & 0.1 & 0.0 & 0.1 & 0.0 & 0.1 & 0.1 \\
\bottomrule
\end{tabular}
}
\\
\vspace{10pt}
UTKFace
\\

\renewcommand{\arraystretch}{1.1}
        \rowcolors{3}{light-light-gray}{white}
\begin{tabular}{llccccccc}
\toprule
\multirow{2}{*}{Track} & & \multicolumn{5}{c}{Race} \\
& & Black & White & Indian & Asian & Other \\
\midrule
\cellcolor{white}& Crime-related & 6.8 & 16.1 & 9.1 & 6.9 & 13.9 \\
\cellcolor{white}\multirow{-2}{*}{Filtering} & Non-human & 0.0 & 0.2 & 0.0 & 0.1 & 0.0 \\
\midrule
\cellcolor{white} & Crime-related & 12.8 & 10.8 & 15.2 & 13.2 & 18.6 \\
\cellcolor{white}\multirow{-2}{*}{\byod}& Non-human & 0.0 & 0.2 & 0.0 & 0.0 & 0.0 \\
\bottomrule
\end{tabular}
\label{tab:app_fairness_harm}
\end{table*}


\clearpage

\section{Extra figures and tables}
\label{sec:app-more-plots}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/scaling_scatter.pdf}
    \caption{Improving downstream performance at smaller scales correlates positively with performance gains at larger scales. These trends suggests that dataset filtering can be studied effectively at smaller scales, even with less computational resources.}
    \label{fig:scaling-scatter-full}
\end{figure}




\begin{table}
\caption{Rank correlation between the performance obtained with various filtering strategies at two different scales. Our experimental suggest that the ranking is relatively consistent between scales, especially for the adjacent scale pairs.}
\setlength\tabcolsep{6pt}
\renewcommand{\arraystretch}{1.1}
\footnotesize
\centering
\begin{tabular}{lccc}
\toprule
Metric   & {\small \texttt{small}} vs  {\small \texttt{medium}} & {\small \texttt{small}} vs {\small \texttt{large}} & {\small \texttt{medium}} vs {\small \texttt{large}} \\
\midrule
ImageNet acc. & 0.901 & 0.830 & 0.863 \\
Average pref. metric & 0.862 & 0.738 & 0.889 \\
\bottomrule
\end{tabular}
\label{tab:correlation}
\end{table}

\begin{figure}
    \centering
    \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=.95\linewidth]{figures/train_samples_small.pdf}
\end{subfigure}
\begin{subfigure}[b]{\textwidth}
    \includegraphics[width=.95\linewidth]{figures/train_samples_large.pdf}
\end{subfigure}
\caption{Performance as a function of the number of training samples from the {\small\texttt{small}} (top) and {\small\texttt{large}} (bottom) scales. There is a significant variance in accuracy even when accounting for the size of the training set.}
    \label{fig:training-samples-extra}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/clip_filter_english.pdf}
    \caption{We examine the percentage of texts classified as English after taking the top fraction (on the x-axis) of the {\small \texttt{large}} billion pool as sorted by CLIP similarity score. We see that doing CLIP filtering implicitly does some English filtering, as image-text pairs with a higher CLIP score are more frequently classified as English.}
    \label{fig:clip_english}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/imagenet_vs_all.pdf}
    \caption{Correlation between ImageNet accuracy and average performance on our suite of evaluation tasks. While ImageNet accuracy strongly correlates with the average performance (both on the clean subset and the full suite), the same is not true for all individual datasets we study, as shown in Appendix \ref{sec:app-more-plots}.}
    \label{fig:imagenet-vs-all}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/robustness.pdf}
    \caption{Zero-shot CLIP models trained with various filtering strategies form a reliable trend relating accuracy on ImageNet and related distribution shifts, exhibiting higher effective robustness when compared to ImageNet-trained models from \citet{taori2020measuring}.}
    \label{fig:robustness}
\end{figure}

\FloatBarrier


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/imagenet_vs_rest_breakdown.pdf}
    \caption{Zero-shot performance on other datasets is often positively correlated with that on ImageNet, but not always. In cases where ImageNet shows close to zero correlation with other datasets, performance on that dataset is often close to random chance.}
    \label{fig:imagenet-vs-all-breakdown}
\end{figure}
\thispagestyle{empty}


\label{app:full_results}
\begin{table*}

        \rowcolors{3}{light-light-gray}{white}
    \small
    \centering
    \caption{Baseline results for the filtering track, {\small \texttt{small}} scale.}
\resizebox{\textwidth}{!}{
    \begin{tabular}{lccccccc}\toprule
         \multirow{2}{*}{Filtering}   & Training & \multirow{2}{*}{ImageNet} & ImageNet & \multirow{2}{*}{VTAB} & \multirow{2}{*}{Retrieval} & Average over\\
        & dataset size &  & dist. shifts &  & & 38 datasets \\\midrule
        No filtering & 12.8M & 0.025 & 0.033 & 0.145 & 0.105 & 0.132  \\% & 0.104 \\ 
        Random subset (75\%) & 9.6M & 0.028 & 0.037 & 0.153 & 0.102 & 0.140  \\% & 0.108\\ 
        Random subset (50\%) & 6.4M & 0.027 & 0.037 & 0.147 & 0.105 & 0.136  \\% & 0.105\\ 
        Random subset (25\%) & 3.2M & 0.022 & 0.032 & 0.130 & 0.094 & 0.126  \\% & 0.097 \\ 
        Random subset (10\%) & 1.3M & 0.010 & 0.018 & 0.116 & 0.075 & 0.102  \\% & 0.069\\ 
        Random subset (1\%) & 128K & 0.002 & 0.005 & 0.095 & 0.049 & 0.078  \\% & 0.048 \\ 
        Caption length & 8.7M & 0.034 & 0.040 & 0.148 & 0.109 & 0.143  \\% & 0.116 \\ 
        Image size & 7.8M & 0.027 & 0.036 & 0.154 & 0.111 & 0.137  \\% & 0.104 \\ 
        English (fasttext) & 6.3M & 0.038 & 0.045 & 0.164 & 0.113 & 0.153  \\% & 0.123\\ 
        English (fasttext) and caption length & 4.8M & 0.041 & 0.048 & 0.159 & 0.111 & 0.153  \\% & 0.125\\ 
        English (fasttext), caption length, and image size & 3.0M & 0.030 & 0.040 & 0.149 & 0.111 & 0.137  \\% & 0.107 \\ 
        English (cld3) & 2.6M & 0.032 & 0.039 & 0.143 & 0.100 & 0.141  \\% & 0.113\\ 
        English (cld3) and caption length & 2.3M & 0.031 & 0.038 & 0.153 & 0.103 & 0.142  \\% & 0.112\\ 
        English (cld3), caption length, and image size & 1.5M & 0.023 & 0.030 & 0.154 & 0.087 & 0.140  \\% & 0.100\\ 
        CLIP B32 score top 1\% & 129K & 0.003 & 0.007 & 0.114 & 0.049 & 0.086  \\% & 0.050\\ 
        CLIP B32 score top 3\% & 384K & 0.006 & 0.014 & 0.104 & 0.054 & 0.089  \\% & 0.060 \\ 
        CLIP B32 score top 10\% & 1.3M & 0.026 & 0.035 & 0.147 & 0.072 & 0.127  \\% & 0.097 \\ 
        CLIP B32 score top 20\% & 2.6M & 0.051 & 0.056 & 0.173 & 0.103 & 0.160  \\% & 0.131\\ 
        CLIP B32 score top 30\% & 3.8M & 0.045 & 0.052 & 0.180 & 0.103 & 0.159  \\% & 0.128\\ 
        CLIP B32 score top 40\% & 5.1M & 0.052 & 0.057 & 0.173 & 0.109 & 0.166  \\% & 0.139\\ 
        CLIP B32 score top 50\% & 6.4M & 0.047 & 0.053 & 0.174 & 0.114 & 0.164  \\% & 0.139\\ 
        CLIP B32 score top 75\% & 9.6M & 0.033 & 0.043 & 0.161 & 0.110 & 0.150  \\%  & 0.122\\ 
        CLIP B32 score top 90\% & 11.5M & 0.028 & 0.039 & 0.140 & 0.108 & 0.136  \\%  & 0.109\\ 
        CLIP B32 threshold at 0.3 + English filter & 942K & 0.022 & 0.032 & 0.138 & 0.073 & 0.121  \\% & 0.089\\ 
        CLIP B32 threshold at 0.28 + English filter & 1.3M & 0.031 & 0.040 & 0.136 & 0.085 & 0.133  \\% & 0.113 \\ 
        CLIP B32 threshold at 0.3 & 2.6M & 0.052 & 0.056 & 0.166 & 0.102 & 0.160  \\% & 0.135\\ 
        CLIP B32 score 1\% to 30\% & 3.7M & 0.053 & 0.058 & 0.185 & 0.102 & 0.170  \\% & 0.139 \\ 
        CLIP B32 score 2\% to 30\% & 3.6M & 0.056 & 0.059 & 0.173 & 0.108 & 0.160  \\% & 0.138\\ 
        CLIP B32 score 5\% to 30\% & 3.2M & 0.052 & 0.055 & 0.177 & 0.104 & 0.168  \\% & 0.137 \\ 
        CLIP L14 score top 1\% & 128K & 0.002 & 0.007 & 0.111 & 0.049 & 0.080  \\% & 0.052 \\ 
        CLIP L14 score top 3\% & 386K & 0.004 & 0.009 & 0.110 & 0.052 & 0.088  \\% & 0.054 \\ 
        CLIP L14 score top 10\% & 1.3M & 0.021 & 0.033 & 0.131 & 0.071 & 0.119  \\% & 0.091 \\ 
        CLIP L14 score top 20\% & 2.6M & 0.042 & 0.051 & 0.165 & 0.100 & 0.151  \\% & 0.122 \\ 
        CLIP L14 score top 30\% & 3.8M & 0.051 & 0.055 & 0.190 & 0.108 & 0.172  \\% & 0.138 \\ 
        CLIP L14 score top 40\% & 5.1M & 0.050 & 0.054 & 0.173 & 0.107 & 0.167  \\% & 0.137 \\ 
        CLIP L14 score top 50\% & 6.4M & 0.045 & 0.052 & 0.164 & 0.110 & 0.159 \\%  & 0.135 \\ 
        CLIP L14 score top 75\% & 9.6M & 0.035 & 0.043 & 0.164 & 0.111 & 0.150  \\% & 0.121\\ 
        CLIP L14 score top 90\% & 11.5M & 0.031 & 0.038 & 0.154 & 0.109 & 0.143  \\% & 0.115 \\ 
        Image-based clustering (ImageNet1k) & 2.9M & 0.043 & 0.047 & 0.178 & 0.112  & 0.158 \\% & 0.123\\ 
        Image-based clustering (ImageNet21k) & 4.5M & 0.035 & 0.045 & 0.154 & 0.112 & 0.146  \\% & 0.119\\ 
        Image-based sampling, $\alpha$=0 & 12.8M & 0.019 & 0.030 & 0.144 & 0.091 & 0.126  \\% & 0.092 \\ 
        Image-based sampling, $\alpha$=0.2 & 12.8M & 0.031 & 0.036 & 0.133 & 0.094 & 0.131 \\%  & 0.106 \\ 
        Image-based sampling, $\alpha$=0.5 & 12.8M & 0.032 & 0.038 & 0.129 & 0.091 & 0.124  \\% & 0.097\\ 
        Image-based sampling, $\alpha$=1 & 12.8M & 0.021 & 0.028 & 0.128 & 0.076 & 0.116  \\% & 0.085 \\ 
        Image-based sampling, $\alpha$=2 & 12.8M & 0.011 & 0.017 & 0.116 & 0.063 & 0.099 \\%  & 0.065\\ 
        ImageNet distance (L14, top 30\%) and English & 2.0M & 0.031 & 0.039 & 0.163 & 0.097 & 0.145  \\% & 0.106\\ 
        ImageNet distance (L14, top 20\%) & 2.6M & 0.030 & 0.035 & 0.155 & 0.096 & 0.136  \\% 0.108\\ 
        ImageNet distance (L14, top 30\%) & 3.9M & 0.034 & 0.041 & 0.151 & 0.099 & 0.138  \\% & 0.114\\ 
        ImageNet distance (L14, top 40\%) & 5.1M & 0.036 & 0.040 & 0.151 & 0.110 & 0.143 \\%  & 0.116\\ 
        Text-based clustering (ImageNet1k) & 427K & 0.009 & 0.016 & 0.120 & 0.055 & 0.096  \\% & 0.061\\ 
        Text-based clustering (ImageNet21k) & 3.2M & 0.046 & 0.052 & 0.169 & 0.112 & 0.156  \\% & 0.128\\ 
        Text-based sampling with average score, $\alpha$=0 & 12.8M & 0.011 & 0.020 & 0.128 & 0.078 & 0.112  \\% & 0.080\\ 
        Text-based sampling with average score, $\alpha$=0.5 & 12.8M & 0.023 & 0.035 & 0.127 & 0.088 & 0.127  \\% & 0.102 \\ 
        Text-based sampling with average score, $\alpha$=1 & 12.8M & 0.040 & 0.044 & 0.163 & 0.105 & 0.154  \\% & 0.124\\ 
        Text-based sampling with average score, $\alpha$=1.2 & 12.8M & 0.038 & 0.045 & 0.150 & 0.101 & 0.142  \\% & 0.112\\ 
        Text-based sampling with max score, $\alpha$=0 & 12.8M & 0.012 & 0.020 & 0.126 & 0.073 & 0.107  \\% & 0.076\\ 
        Text-based sampling with max score, $\alpha$=0.5 & 12.8M & 0.025 & 0.033 & 0.134 & 0.089 & 0.128  \\% & 0.098\\ 
        Text-based sampling with max score, $\alpha$=1 & 12.8M & 0.040 & 0.046 & 0.159 & 0.106 & 0.149  \\% & 0.122\\ 
        Text-based sampling with max score, $\alpha$=1.2 & 12.8M & 0.040 & 0.050 & 0.161 & 0.106 & 0.151  \\% & 0.123\\ 
        Intersect IN1k image clustering and CLIP B32 score top 30\% & 1.4M & 0.049 & 0.053 & 0.150 & 0.095 & 0.147  \\% & 0.125\\ 
        Intersect IN1k image clustering and CLIP L14 score top 30\% & 1.4M & 0.039 & 0.045 & 0.162 & 0.089 & 0.144  \\% & 0.115\\ 
        Intersect IN21k image clustering and CLIP B32 score top 30\% & 2.1M & 0.052 & 0.057 & 0.179 & 0.103 & 0.166  \\% & 0.132\\ 
        Intersect IN21k image clustering and CLIP L14 score top 30\% & 2.1M & 0.047 & 0.053 & 0.176 & 0.101 & 0.162  \\% & 0.131\\ 
        \bottomrule
    \end{tabular}
    }
\label{tab:full-small}
\end{table*}
\newpage
\begin{table*}
 \rowcolors{3}{light-light-gray}{white}
    \small
    \centering
    \caption{Baseline results for the filtering track, {\small \texttt{medium}} scale.}
\resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccc}\toprule
         \multirow{2}{*}{Filtering}   & Training & \multirow{2}{*}{ImageNet} & ImageNet & \multirow{2}{*}{VTAB} & \multirow{2}{*}{Retrieval} & Average over \\
        & dataset size &  & dist. shifts &  & & 38 datasets\\\midrule
        No filtering & 128M & 0.176 & 0.152 & 0.259 & 0.174 & 0.254  \\% & 0.244\\ 
        Random subset (75\%) & 96.0M & 0.175 & 0.154 & 0.265 & 0.174 & 0.254  \\% & 0.240\\ 
        Random subset (50\%) & 64.0M & 0.171 & 0.151 & 0.258 & 0.170 & 0.249  \\% & 0.235 \\ 
        Random subset (25\%) & 32.0M & 0.155 & 0.136 & 0.246 & 0.162 & 0.237  \\% & 0.223\\ 
        Random subset (10\%) & 12.8M & 0.107 & 0.095 & 0.210 & 0.121 & 0.198  \\% & 0.176\\ 
        Random subset (1\%) & 1.3M & 0.009 & 0.017 & 0.102 & 0.064 & 0.090  \\% & 0.064 \\ 
        Caption length & 87.5M & 0.199 & 0.172 & 0.275 & 0.182 & 0.271  \\% & 0.260 \\ 
        Image size & 77.8M & 0.189 & 0.163 & 0.248 & 0.182 & 0.255  \\% & 0.243\\ 
        English (fasttext) & 63.0M & 0.214 & 0.182 & 0.290 & 0.188 & 0.280  \\% & 0.270\\ 
        English (fasttext) and caption length & 47.8M & 0.226 & 0.193 & 0.297 & 0.192 & 0.289  \\% & 0.280\\ 
        English (fasttext), caption length, and image size & 29.8M & 0.226 & 0.193 & 0.284 & 0.192 & 0.280  \\% & 0.271 \\ 
        English (cld3) & 25.6M & 0.200 & 0.175 & 0.296 & 0.181 & 0.275  \\% & 0.262\\ 
        English (cld3) and caption length & 22.9M & 0.204 & 0.175 & 0.287 & 0.181 & 0.273  \\% & 0.263 \\ 
        English (cld3), caption length, and image size & 14.6M & 0.179 & 0.159 & 0.243 & 0.167 & 0.243  \\% & 0.232\\ 
        CLIP B32 score top 1\% & 1.3M & 0.025 & 0.037 & 0.140 & 0.072 & 0.125  \\% & 0.090\\ 
        CLIP B32 score top 3\% & 3.9M & 0.093 & 0.096 & 0.205 & 0.103 & 0.186  \\% & 0.164\\ 
        CLIP B32 score top 10\% & 12.8M & 0.231 & 0.199 & 0.305 & 0.152 & 0.294  \\% & 0.284 \\ 
        CLIP B32 score top 20\% & 25.7M & 0.279 & 0.234 & 0.337 & 0.178 & 0.325  \\% & 0.318 \\ 
        CLIP B32 score top 30\% & 38.4M & 0.285 & 0.240 & 0.355 & 0.187 & 0.333  \\% & 0.327\\ 
        CLIP B32 score top 40\% & 51.3M & 0.273 & 0.227 & 0.333 & 0.193 & 0.318  \\% & 0.312\\ 
        CLIP B32 score top 50\% & 64.0M & 0.256 & 0.219 & 0.322 & 0.196 & 0.311  \\% & 0.305\\ 
        CLIP B32 score top 75\% & 96.1M & 0.211 & 0.180 & 0.301 & 0.185 & 0.285  \\% & 0.276\\ 
        CLIP B32 score top 90\% & 115M & 0.189 & 0.165 & 0.279 & 0.178 & 0.270  \\% & 0.253\\ 
        CLIP B32 threshold at 0.3 + English filter & 9.4M & 0.208 & 0.184 & 0.292 & 0.156 & 0.272  \\% & 0.259\\ 
        CLIP B32 threshold at 0.28 + English filter & 13.0M & 0.230 & 0.198 & 0.307 & 0.170 & 0.287  \\% & 0.276 \\ 
        CLIP B32 threshold at 0.3 & 25.9M & 0.282 & 0.233 & 0.340 & 0.178 & 0.327  \\% & 0.322 \\ 
        CLIP B32 score 1\% to 30\% & 37.1M & 0.287 & 0.238 & 0.347 & 0.187 & 0.329  \\% & 0.325\\ 
        CLIP B32 score 2\% to 30\% & 35.9M & 0.288 & 0.238 & 0.338 & 0.184 & 0.325  \\% & 0.322\\ 
        CLIP B32 score 5\% to 30\% & 32.0M & 0.281 & 0.230 & 0.352 & 0.187 & 0.334  \\% & 0.319\\ 
        CLIP L14 score top 1\% & 1.3M & 0.014 & 0.025 & 0.136 & 0.059 & 0.109  \\% & 0.075\\ 
        CLIP L14 score top 3\% & 3.9M & 0.065 & 0.077 & 0.176 & 0.088 & 0.158  \\% & 0.139 \\ 
        CLIP L14 score top 10\% & 12.8M & 0.198 & 0.183 & 0.283 & 0.142 & 0.274  \\% & 0.261\\ 
        CLIP L14 score top 20\% & 25.7M & 0.260 & 0.225 & 0.326 & 0.173 & 0.317  \\% & 0.310 \\ 
        CLIP L14 score top 30\% & 38.4M & 0.273 & 0.230 & 0.338 & 0.183 & 0.323  \\% & 0.319 \\ 
        CLIP L14 score top 40\% & 51.2M & 0.262 & 0.226 & 0.330 & 0.192 & 0.322  \\% & 0.318 \\ 
        CLIP L14 score top 50\% & 64.1M & 0.254 & 0.218 & 0.322 & 0.199 & 0.310  \\% & 0.303\\ 
        CLIP L14 score top 75\% & 96.1M & 0.212 & 0.180 & 0.287 & 0.190 & 0.281  \\% & 0.272\\ 
        CLIP L14 score top 90\% & 115M & 0.188 & 0.164 & 0.258 & 0.178 & 0.262  \\% & 0.254 \\ 
        Image-based clustering (ImageNet1k) & 29.2M & 0.268 & 0.213 & 0.319 & 0.193 & 0.307  \\% & 0.295\\ 
        Image-based clustering (ImageNet21k) & 45.1M & 0.238 & 0.198 & 0.304 & 0.193 & 0.292  \\% & 0.280 \\ 
        Image-based sampling, $\alpha$=0 & 128M & 0.170 & 0.150 & 0.266 & 0.162 & 0.250   \\% & 0.235\\ 
        Image-based sampling, $\alpha$=0.2 & 128M & 0.249 & 0.193 & 0.292 & 0.168 & 0.280 \\%  & 0.272\\ 
        Image-based sampling, $\alpha$=0.5 & 128M & 0.269 & 0.196 & 0.301 & 0.163 & 0.280  \\% & 0.276 \\ 
        Image-based sampling, $\alpha$=1 & 128M & 0.207 & 0.145 & 0.264 & 0.130 & 0.236 \\%  & 0.222\\ 
        Image-based sampling, $\alpha$=2 & 128M & 0.118 & 0.082 & 0.207 & 0.094 & 0.179 \\%  & 0.156 \\ 
        ImageNet distance (L14, top 30\%) and English & 19.8M & 0.212 & 0.158 & 0.272 & 0.148 & 0.257  \\% & 0.244 \\ 
        ImageNet distance (L/14, top 20\%) & 25.8M & 0.193 & 0.138 & 0.276 & 0.149 & 0.250  \\% & 0.234\\ 
        ImageNet distance (L/14, top 30\%) & 38.5M & 0.212 & 0.159 & 0.283 & 0.165 & 0.266  \\% & 0.253\\ 
        ImageNet distance (L/14, top 40\%) & 51.3M & 0.212 & 0.165 & 0.273 & 0.171 & 0.267  \\% & 0.254\\ 
        Text-based clustering (ImageNet1k) & 4.3M & 0.099 & 0.090 & 0.173 & 0.095 & 0.165  \\% & 0.144\\ 
        Text-based clustering (ImageNet21k) & 31.7M & 0.255 & 0.215 & 0.328 & 0.183 & 0.301 \\%  & 0.292\\ 
        Text-based sampling with average score, $\alpha$=0 & 128M & 0.136 & 0.110 & 0.213 & 0.114 & 0.207 \\%  & 0.190\\ 
        Text-based sampling with average score, $\alpha$=0.5 & 128M & 0.222 & 0.178 & 0.273 & 0.157 & 0.265  \\% & 0.257\\ 
        Text-based sampling with average score, $\alpha$=1 & 128M & 0.245 & 0.204 & 0.302 & 0.189 & 0.289  \\% & 0.282\\ 
        Text-based sampling with average score, $\alpha$=1.2 & 128M & 0.231 & 0.200 & 0.298 & 0.182 & 0.284  \\% & 0.277\\ 
        Text-based sampling with max score, $\alpha$=0 & 128M & 0.140 & 0.116 & 0.242 & 0.114 & 0.223  \\% & 0.201\\ 
        Text-based sampling with max score, $\alpha$=0.5 & 128M & 0.229 & 0.190 & 0.290 & 0.155 & 0.279  \\% & 0.264\\ 
        Text-based sampling with max score, $\alpha$=1 & 128M & 0.247 & 0.209 & 0.300 & 0.183 & 0.290  \\% & 0.282\\ 
        Text-based sampling with max score, $\alpha$=1.2 & 128M & 0.235 & 0.200 & 0.298 & 0.178 & 0.285  \\% & 0.279\\
        Intersect IN1k image clustering and CLIP B32 score top 30\% & 14.2M & 0.305 & 0.243 & 0.342 & 0.182 & 0.322  \\% & 0.315\\ 
        Intersect IN1k image clustering and CLIP L14 score top 30\% & 14.0M & 0.297 & 0.239 & 0.346 & 0.170 & 0.323  \\% & 0.317\\ 
        Intersect IN21k image clustering and CLIP B32 score top 30\% & 21.1M & 0.298 & 0.244 & 0.347 & 0.184 & 0.330  \\% & 0.328\\ 
        Intersect IN21k image clustering and CLIP L14 score top 30\% & 20.8M & 0.290 & 0.241 & 0.339 & 0.182 & 0.323  \\% & 0.319\\
        \bottomrule
    \end{tabular}
}
    \label{tab:full-medium}
\end{table*}
\newpage
\begin{table*}
 \rowcolors{3}{light-light-gray}{white}
    \small
    \centering
    \caption{Baseline results for the filtering track, {\small \texttt{large}} scale.}
\resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccc}\toprule
         \multirow{2}{*}{Filtering}   & Training & \multirow{2}{*}{ImageNet} & ImageNet & \multirow{2}{*}{VTAB} & \multirow{2}{*}{Retrieval} & Average over\\
        & dataset size &  & dist. shifts &  & & 38 datasets\\\midrule
        No filtering & 1.28B & 0.459 & 0.378 & 0.426 & 0.305 & 0.428 \\% && 0.441\\
        Random subset (75\%) & 960M & 0.456 & 0.379 & 0.435 & 0.302 & 0.434 \\% && 0.443 \\
        Random subset (50\%) & 640M & 0.453 & 0.377 & 0.427 & 0.298 & 0.424 \\% && 0.434 \\
        Random subset (25\%) & 320M & 0.447	& 0.373 & 0.424 & 0.294 & 0.425 \\% && 0.439 \\
        Random subset (10\%) & 128M & 0.426 & 0.350 & 0.417 & 0.286 & 0.414 \\% && 0.416 \\ 
        Random subset (1\%) & 12.8M & 0.135 & 0.118 & 0.219 & 0.105 & 0.216 \\% && 0.193 \\ 
        Caption length & 874M & 0.474 & 0.392 & 0.438 & 0.322 & 0.435 \\% && 0.449\\ 
        Image size & 777M & 0.466 & 0.375 & 0.421 & 0.316 & 0.419 \\% && 0.424\\ 
        English (fasttext) & 630M & 0.500 & 0.414 & 0.449 & 0.337 & 0.452 \\% && 0.468\\ 
        English (fasttext), caption length, and image size & 298M & 0.516 & 0.423 & 0.446 & 0.353 & 0.448 \\% && 0.458\\ 
        English (cld3) & 256M & 0.486 & 0.405 & 0.462 & 0.343 & 0.448 \\% && 0.464\\ 
        CLIP B32 score top 10\% & 128M & 0.543 & 0.440 & 0.471 & 0.307 & 0.473 \\% && 0.502 \\ 
        CLIP B32 score top 20\% & 257M & 0.578 & 0.465 & 0.516 & 0.338 & 0.505 \\% && 0.528 \\ 
        CLIP B32 score top 30\% & 384M & 0.578 & 0.466 & 0.525 & 0.349 & 0.517 \\% && 0.533\\ 
        CLIP B32 score top 40\% & 512M & 0.560 & 0.454 & 0.512 & 0.352 & 0.501 \\% && 0.527\\ 
        CLIP B32 score top 50\% & 640M & 0.546 & 0.450 & 0.504 & 0.353 & 0.494 \\% && 0.519\\ 
        CLIP B32 threshold at 0.3 + English filter & 94.3M & 0.553 & 0.447 & 0.511 & 0.351 & 0.491 \\% && 0.500 \\ 
        CLIP B32 threshold at 0.28 + English filter & 130M & 0.553 & 0.453 & 0.510 & 0.365 & 0.491 \\% && 0.509 \\ 
        CLIP B32 threshold at 0.3 & 258M & 0.579 & 0.464 & 0.501 & 0.338 & 0.495 \\% && 0.519 \\ 
        CLIP L14 score top 10\% & 128M & 0.528 & 0.444 & 0.482 & 0.293 & 0.477 \\% && 0.497 \\ 
        CLIP L14 score top 20\% & 257M & 0.570 & 0.466 & 0.524 & 0.331 & 0.511 \\% & 0.531 \\ 
        CLIP L14 score top 30\% & 384M & 0.578 & 0.474 & 0.538 & 0.342 & 0.520 \\% & 0.542\\ 
        CLIP L14 score top 40\% & 512M & 0.564 & 0.462 & 0.533 & 0.346 & 0.520 \\% & 0.538 \\ 
        CLIP L14 score top 50\% & 641M & 0.548 & 0.455 & 0.539 & 0.345 & 0.518 \\% & 0.534\\ 
        Image-based clustering (ImageNet1k) & 294M & 0.572 & 0.454 & 0.483 & 0.353 & 0.471 \\% & 0.487\\ 
        Image-based clustering (ImageNet21k) & 450M & 0.527 & 0.433 & 0.468 & 0.337 & 0.461 \\% & 0.474 \\ 
        Text-based clustering (ImageNet1k) & 42.7M & 0.419 & 0.355 & 0.340 & 0.210 & 0.353 \\% & 0.357\\ 
        Text-based clustering (ImageNet21k) & 317M & 0.561 & 0.465 & 0.465 & 0.352 & 0.466 \\% & 0.484 \\
        Intersect IN1k image clustering and CLIP B32 score top 30\% & 143M & 0.632 & 0.498 & 0.525 & 0.371 & 0.517 \\%  & 0.541\\ 
        Intersect IN1k image clustering and CLIP L14 score top 30\% & 140M & 0.631 & 0.508 & 0.546 & 0.369 & 0.527 \\%  & 0.550\\ 
        Intersect IN21k image clustering and CLIP B32 score top 30\% & 211M & 0.605 & 0.481 & 0.531 & 0.363 & 0.509  \\% & 0.532\\ 
        Intersect IN21k image clustering and CLIP L14 score top 30\% & 208M & 0.506 & 0.416 & 0.466 & 0.300 & 0.461  \\% & 0.488\\ 
 \bottomrule 
    \end{tabular}
    }
    \label{tab:full-large}
\end{table*}

\begin{table*}
 \rowcolors{3}{light-light-gray}{white}
  \rowcolors{3}{light-light-gray}{white}
    \small
    \centering
    \caption{Baseline results for the filtering track, {\small \texttt{xlarge}} scale.}
\resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccc}\toprule
         \multirow{2}{*}{Filtering}   & Training & \multirow{2}{*}{ImageNet} & ImageNet & \multirow{2}{*}{VTAB} & \multirow{2}{*}{Retrieval} & Average over \\
        & dataset size &  & dist. shifts &  & & 38 datasets\\\midrule
        No filtering & 12.8B & 0.723 & 0.612 & 0.611 & 0.441 & 0.611  \\% & 0.648\\
        CLIP B32 score top 30\% & 3.84B & 0.764 & 0.640 & 0.628 & 0.474 & 0.628  \\% & 0.672\\ 
        CLIP B32 threshold at 0.28 + English filter & 1.3B & 0.755 & 0.637 & 0.624 & 0.503 & 0.627 \\%  & 0.666 \\ 
        CLIP L14 score top 20\% & 2.56B & 0.761 & 0.649 & 0.630 & 0.452 & 0.626 \\%  & 0.668 \\ 
        CLIP L14 score top 25\% & 3.2B & 0.768 & 0.656 & 0.621 & 0.465 & 0.628  \\% & 0.676\\ 
        CLIP L14 score top 30\% & 3.84B & 0.764 & 0.655 & 0.643 & 0.468 & 0.641  \\% & 0.681 \\
        Intersect IN1k image clustering and CLIP L14 score top 30\% & 1.38B & 0.792 & 0.679 & 0.652 & 0.489 & 0.653 \\%  & 0.695\\ 
        \bottomrule 
    \end{tabular}
    }
    \label{tab:full-xlarge}
\end{table*}

\clearpage
\input{datasheet}